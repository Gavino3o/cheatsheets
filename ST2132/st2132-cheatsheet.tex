\documentclass[10pt, landscape]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{calc}
\usepackage{multicol}
\usepackage[a4paper,margin=3mm,landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\usepackage{newtxtext} 
\usepackage{enumitem}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\setlist{nosep}
% for including images
\graphicspath{ {./images/} }

\pdfinfo{
  /Title (ST2132.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Jovyn Tan)
  /Subject (ST2132)
/Keywords (ST2132, nus,cheatsheet,pdf)}

% Turn off header and footer
\pagestyle{empty}

% redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
  {-1ex plus -.5ex minus -.2ex}%
  {0.5ex plus .2ex}%x
{\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
  {-1explus -.5ex minus -.2ex}%
  {0.5ex plus .2ex}%
{\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
  {-1ex plus -.5ex minus -.2ex}%
  {1ex plus .2ex}%
{\normalfont\small\bfseries}}%
\makeatother

\renewcommand{\familydefault}{\sfdefault}
\renewcommand\rmdefault{\sfdefault}
%  makes nested numbering (e.g. 1.1.1, 1.1.2, etc)
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\renewcommand\labelitemii{•}
\renewcommand\labelitemiii{•}

\definecolor{mathblue}{cmyk}{1,.72,0,.38}
\everymath\expandafter{\the\everymath \color{mathblue}}

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
%% adjust spacing for all itemize/enumerate
\setlength{\leftmargini}{0.5cm}
\setlength{\leftmarginii}{0.5cm}
\setlist[itemize,1]{leftmargin=2mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,2]{leftmargin=4mm,labelindent=1mm,labelsep=1mm}

\newcommand{\cov}{\mathop{\mathrm{cov}}}
\newcommand{\var}{\mathop{\mathrm{var}}}

% adding my commands
\input{../commands/style-helpers.tex}
\input{../commands/math.tex}

% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols*}{4}
  % multicol parameters
  \setlength{\columnseprule}{0.25pt}

  \begin{center}
    \fbox{%
      \parbox{0.8\linewidth}{\centering \textcolor{black}{
          {\Large\textbf{ST2132}}
        \\ \normalsize{AY23/24 SEM 1}}
        \\ {\footnotesize \textcolor{gray}{github/jovyntls}}
      }%
    }
  \end{center}

  \section{01. PROBABILITY}

  \begin{itemize}
    \item \definition[of an event]{probability} the limiting relative frequency of its occurrence as the experiment is repeated many times
    \item the \textbf{realisation} $x$ is a constant, and $X$ is a generator
      \begin{itemize}
        \item running $r$ experiments gives us $r$ realisations $x_1, \dots, x_r$
      \end{itemize}
  \end{itemize}

  \subsection{expectation}

  \begin{tightcenter}
    \begin{multicols*}{2}
      \textbf{discrete}: \\* (mass function) \\* \( {\displaystyle{ E(X) := \sum^n_{i=1}x_ip_i }} \) 

      \textbf{continuous}: \\* (density function) \\* \( {\displaystyle{ E(X) := \int^\infty_{-\infty} xf(x) \dx }} \) 
    \end{multicols*}
  \end{tightcenter}

  \subsubsection{expectation of a function $h(X)$}

  $ E\{h(X)\} = \begin{cases}
        \sum^n_{i=1} h(x_i)p_i &\text{$X$ is discrete} \\ 
        \int^\infty_{-\infty} h(x)f(x)\dx &\text{$X$ is continuous} \\ 
      \end{cases}$ 

  \subsection{variance}

  \begin{tightcenter}
    \begin{align*}
      \textbf{variance}, \var(X) &:= E\{(X-\mu)^2\}
                        \\ &= E(X^2) - E(X)^2
      \\ \textbf{standard deviation}, SD(X) &:= \sqrt{\var(X)}
    \end{align*}
  \end{tightcenter}

  \subsection{law of large numbers}

  \begin{tightcenter}
    \textbf{LLN:} for a function $h$, as number of realisations $r \to \infty$, 

    $\bar{x} \to E(X)$, $v \to \var(X)$

    $ \frac{1}{r} \sum^r_{i=1}h(x_i) \to E\{h(X)\} $
  \end{tightcenter}

  mean of realisations,  \( {\displaystyle{ \bar{x} := \frac{1}{r} \sum^r_{i=1} x_i }} \) 

  variance of realisations, \( {\displaystyle{ v := \frac{1}{r}\sum^r_{i=1} (x_i - \bar{x})^2 }} \) 

  \subsection{Monte Carlo approximation}

  \begin{tightcenter}
    \( {\displaystyle{ E\{h(X)\} \approx \frac{1}{r} \sum^r_{i=1} h(x_i) }} \) 
  \end{tightcenter}

  by LLN, as $r \to \infty$, the approximation becomes exact

  \subsection{joint distribution}

  \begin{itemize}
    \item \textbf{discrete}: mass function \\* \( {\displaystyle{\text{Pr} ( X = x_i, Y = y_j ) = p_{ij}}} \)  where $x_1, \dots, x_i$ and $y_1, \dots, y_j$ are all possible values of $X$  and $Y$
    \item \textbf{continuous}: density function \\* \( {\displaystyle{f : \mathbb{R}^2  \to [0, \infty), \int^\infty_{-\infty} f(x, y) \dx \dy = 1}} \) 
  \end{itemize}

  \begin{tightcenter}
    for $h : \mathbb{R}^2 \to \mathbb{R}$,
    \\* \( {\displaystyle{ E\{ h(X,Y)\} = }} \) 
    \( {\displaystyle{ 
        \begin{cases}
          \sum^I_{i=1}\sum^J_{j=1} h(x_i, y_j) p_{ij} & X \text{ is discrete}
          \\ \int^\infty_{-\infty}\int^\infty_{-\infty} h(x, y) f(x, y) \dx \dy & Y \text{ is continuous}
        \end{cases}
    }} \) 
  \end{tightcenter}

  \subsection{algebra of RV's}

  let $X, Y$ be RVs and $a, b, c$ be constants

  \begin{itemize}
    \item $Z = aX + bY + c$ is also an RV
      \begin{itemize}
        \item $z = ax + by + c$ is a realisation of $Z$
      \end{itemize}
    \item linearity of expectation - $E(Z) = aE(X) + bE(Y) + c$
  \end{itemize}

  \subsection{covariance}

  let $\mu_X = E(X)$, $\mu_Y=E(Y)$.

  \begin{tightcenter}
    \textbf{covariance}, $\cov(X,Y) = E\{ (X-\mu_X)(Y-\mu_Y) \}$
  \end{tightcenter}

  \begin{itemize}
    \item $\cov(X, Y) = E(XY) - \mu_X\mu_Y$
    \item $\cov(X, Y) = \cov(Y,X)$
    \item $\cov(X, X) = \var(X)$
    \item $\cov(W, aX+bY+c) = a\cov(W, X) + b\cov(W, Y)$
    \item $\var(aX + bY + c) = a^2 \var(X) + b^2 \var(Y) + 2ab\cov(X, Y)$
  \end{itemize}

  \subsection{joint, marginal \& conditional distributions}

  let $f(x, y)$ be the \textbf{joint} density and $f_X(x), f_Y(y)$ be the \textbf{marginal} densities. then

  \begin{tightcenter}
    $f(x, y) = f_X(x)f_Y(y \vert x) = f_Y(y) f_X(x \vert y)$,  $\quad x, y \in \mathbb{R}$
  \end{tightcenter}

  $f_Y(\cdot \vert x)$ is the \textbf{conditional} density of $Y$ given $X=x$
  $f_X(\cdot \vert y)$ is the \textbf{conditional} density of $X$ given $Y=y$ 

  \subsection{independence}

  $X, Y$ are independent $\iff \forall x, y \in \mathbb{R}$, 

  \begin{enumerate}
    \item $f(x, y) = f_X(x)f_Y(y)$ 
    \item $f_Y(y\vert x) = f_Y(y)$
    \item $f_X(x\vert y) = f_Y(x)$
  \end{enumerate}

  $X, Y$ are independent  $\Rightarrow$
  \begin{itemize}
    \item $E(XY) = E(X)E(Y)$ 
    \item $\cov(X,Y) = 0$
  \end{itemize}
  (the converse does not hold)

  \subsection{Distributions}

  if $X$ is iid, then $\var(\sum^n_{i=-1} x_i) = \sum^n_{i=1} \var(x_i)$

  \subsubsection{bernoulli}

  \begin{itemize}
    \item $X \sim Bernoulli(p) \quad \Rightarrow$ coin flip with probability $p$
  \end{itemize}

  \subsubsection{binomial}

  \begin{itemize}
    \item $X \sim Bin(n, p) \quad \Rightarrow$ $n$ coin flips with probability $p$
    \item $X_i \mathop{\sim}\limits^{i.i.d.} Bernoulli(p)$
    \item $E(X) = \sum^n_{k=1} k \binom{n}{k} p^k (1-p)^{n-k}$ 
  \end{itemize}

  \begin{tightcenter}
    $E(X) = np, \quad \var(X) = np(1-p)$
  \end{tightcenter}

  \subsubsection{multinomial}

   \begin{itemize}
    \item $X \sim Multinomial(n, \mathbf{p})$ $\quad \Rightarrow$ $n$ runs of an experiment with $k$ outcomes with probability vector $\mathbf{p}$
      \begin{itemize}
        \item An experiment with $k$ outcomes $E_1, \dots, E_k$, $\; Pr(E_i) = p_i$.
          For some $1 \leq i \leq k$, let $X_i$ be the number of times $E_i$ occurs in $n$ runs.
      \end{itemize}
  \end{itemize}

  \begin{tightcenter}
    $(X_1, \dots, X_k)$ has the multinomial distribution:
    \( {\displaystyle{ Pr(X_1 = x_1, \dots, X_k = x_k) = \binom{n}{x_1 \dots x_k} \Pi^k_{i=1} p_i^{x_i} }} \)   
  \end{tightcenter}

  \begin{itemize}
    \item combinatorially, $\binom{n}{x_1 \dots x_k} = \frac{n!}{x_1! x_2! \dots x_k!}$
  \end{itemize}

  \begin{tightcenter}
    $E(X) = \begin{bmatrix}np_1 \\ np_2 \\ \vdots \\ np_k\end{bmatrix}$,  
    $\quad \var(X_i) = np_i (1-p_i)$

    $\var(X) =$ \textit{covariance matrix} $M$ with $m_{ij} = \begin{cases}
      \var(X_i) & \text{if }i=j
      \\ \cov(X_i, X_j) & \text{if }i \neq j
    \end{cases} $

  \end{tightcenter}

  \begin{itemize}
    \item $\cov(X_i, X_j) < 0$
    \item $X_i \sim Bin(n, p_i)$
      \begin{itemize}
        \item $E(X_i) = np_i, \quad \var(X_i) = np_i (1-p_i)$
      \end{itemize}
    \item $X_i + X_j \sim Bin(n, p_i + p_j)$
      \begin{itemize}
        \item $\var(X_i + X_j) = n(p_i + p_j) (1-p_i-p_j)$
      \end{itemize}
  \end{itemize}

  \subsection{Conditional expectation}

  \subsubsection{discrete case}

  for r.v.s (X, Y), let $f_Y(\cdot \vert x_i)$ be the conditional mass function of $Y$ given $X = x_i$.

  \begin{tightcenter}
    \( {\displaystyle{ E[Y \vert x_i] := \sum^J_{j=1} y_j f_Y (y_j \vert x_i) }} \) 

    \( {\displaystyle{ \var[Y\vert x_i] := \sum^J_{j=1} (y_j - E[Y\vert x_i])^2 f_Y (y_j \vert x_i) }} \) 
  \end{tightcenter}

  $E[Y\vert x_i]$ is like $E(Y)$, with conditional distribution replacing marginal distribution  $f_Y(\cdot)$. 
  likewise $\var[Y\vert x_i]$ is like $\var(Y)$

  \subsubsection{continuous case}

  \begin{tightcenter}
    \( {\displaystyle{ E[Y \vert x] := \int^\infty_{-\infty} y f_Y (y \vert x) \dy }} \) 

    \( {\displaystyle{ \var[Y\vert x] := \int^\infty_{-\infty} (y - E[Y\vert x])^2 f_Y (y \vert x) \dy }} \) 
  \end{tightcenter}


  \section{02. PROBABILITY (2)}

  \subsection{mean square error (MSE)}

  \begin{tightcenter}
    \textbf{mean square error}, $MSE = E\{ (Y-c)^2 \}$
  \end{tightcenter}

  \begin{itemize}
    \item $MSE = \var(Y) + \{ E(Y) - c\}^2$
    \item $Y$ and $X$ are correlated: 
      \\* $MSE = \var[Y \vert x] + \{ E[Y\vert x] - c \}^2$
      \\* $MSE = E[(Y-c)^2 \vert x] = E[\{ Y - E(Y) \}^2 \vert x]$
      \begin{itemize}
        \item to predict $Y$, choose $c$ that depends on $x$
      \end{itemize}
  \end{itemize}

  \subsubsection{random conditional expectations}

  let $X, Y$ be r.v.s.
  \begin{itemize}
    \item $E[Y \vert X]$ is a r.v. which takes value $E[Y \vert x]$ with probability/density $f_X(x)$
    \item $\var[Y \vert X]$ is a r.v. which takes value $\var[Y \vert x]$ with probability/density $f_X(x)$ 
    \item $E(E[X_2 \vert X_1]) = E(X_2)$
    \item $\var(E[X_2 \vert X_1]) + E(\var[X_2 \vert X_1]) = \var(X_2)$
  \end{itemize}

  \subsubsection{mean MSE}

  \begin{tightcenter}
    $\frac{1}{n} \sum^n_{i=1}\var[Y \vert x_i] \approx TODO$
  \end{tightcenter}

  \subsection{cumulative distribution function (cdf)}

  for r.v. $X$, let $F(x) = P(X \leq x)$

  \begin{itemize}
    \item domain: $\mathbb{R}$; $\;$ codomain: $[0,1]$
  \end{itemize}

  \begin{tightcenter}
    $F(x) = \int^\infty_{-\infty} f(x) \dx$
  \end{tightcenter}

  \subsection{standard normal distribution}

  \begin{tightcenter}
    $Z \sim N(0, 1)$ has density function \\* 
    \( {\displaystyle{ \phi(z) = \frac{1}{\sqrt{2\pi}} \exp \{ -\frac{z^2}{2} \}, \quad -\infty < z < \infty }} \) 
  \end{tightcenter}

  \begin{itemize}
    \item $E(Z) = 0, \quad \var(Z) = 1$
      \begin{itemize}
        \item  $E(Z) = \int^\infty_{-\infty} z\phi(z) \mathop{dz}$
        \item  $E(Z^2) = \int^\infty_{-\infty} z^2\phi(z) \mathop{dz}$
        \item $E(Z^k) = \begin{cases} 0 & \text{if $k$ is odd} \\ 1 & \text{if $k$ is even} \end{cases} $
      \end{itemize}
    \item CDF, $\Phi(x) = P(Z \leq x), \; x \in \mathbb{R}$
      \begin{itemize}
        \item $\Phi(x) = \int^x_{-\infty} \phi(z) \mathop{dz}$
      \end{itemize}
  \end{itemize}

  \subsubsection{general normal distribution}

  let  $X \sim N(\mu, \sigma^2)$ and $Y \sim N(\nu, \tau^2)$

  \begin{tightcenter}
    \textbf{standardisation:} $\frac{X - \mu}{\sigma} \sim N(0, 1)$
  \end{tightcenter}

  \begin{itemize}
    \item summations: 
      \begin{itemize}
        \item for constants $a, b \neq 0$, 
          \\* $\quad \; a + bX \sim N(a+b\mu, \; b^2\sigma^2)$
        \item $X + Y \sim N(\mu + \nu, \; \sigma^2 + \tau^2 + 2\cov(X,Y))$
          \begin{itemize}
            \item $\cov(X, Y) = 0$, $\quad \Rightarrow \quad$ $X \perp Y$
            \item $X \perp Y$ $\quad \Rightarrow \quad$ $X + Y \sim N(\mu + \nu, \; \sigma^2 + \tau^2)$
          \end{itemize}
      \end{itemize}

    \item for $W = a + bX$, 
      \begin{itemize}
        \item density  $f_W(w) = \frac{d}{dw} F_W(w)$
        \item cdf $F_W(w) = P(X \leq \frac{w-a}{b}) = \Phi(\frac{w-a}{b})$
      \end{itemize}
  \end{itemize}

  \subsection{Central limit theorem}

  let $X_1, \dots, X_n$ be iid rv's with expectation $\mu$ and SD $\sigma$, with $ S_n \sum^n_{i=1} X_i $

  \begin{tightcenter}
    \textbf{CLT} 
    \\* as $n \to \infty$, the distribution of the standardised $S_n = \frac{S_n - n\mu}{\sqrt{n}\sigma}$ converges to $N(0,1)$
  \end{tightcenter}

  \begin{itemize}
    \item $E(S_n) = n\mu$, $\; \var(S_n) = n\sigma^2$
    \item for large $n$, approximately $S_n \sim N(n\mu, n\sigma^2)$
  \end{itemize}

  \subsubsection{Bernoulli}

  let $X_i \sim Bernoulli(p)$. then
  \begin{itemize}
    \item $S_n \sim Binom(n, p)$
      \begin{itemize}
        \item for large $n$, $S_n = N(np, np(1-p))$
      \end{itemize}
    \item CLT: standardised $\frac{S_n - np}{\sqrt{n}\sqrt{p(1-p)}} \to N(0,1)$ as $n \to \infty$
  \end{itemize}

  \subsection{$\chi^2$ RVs}

  let $Z \sim N(0,1)$. 
  \begin{tightcenter}
    $Z^2 \sim \chi^2_1$ \\*
    $Z^2$ has $\chi^2$ distribution with 1 degree of freedom
    \\* $E(Z^2) = 1$
    \\* $\var(Z^2) = E(Z^4) - \{ E(Z^2) \}^2 = 2$
  \end{tightcenter}

  let $V_1, \dots, V_n$ be iid $\chi^2_1$ RVs. then 
  \begin{itemize}
    \item $V = \sum^n_{i=1}V_i$ has a $\chi^2_n$ distribution: $V \sim \chi^2_n$
    \item $E(V) = n \quad \var(V) = 2n$
  \end{itemize}

  \subsection{Gamma distribution}

  \begin{tightcenter}
    let $\alpha, \lambda > 0$. The $Gamma(\alpha, \lambda)$ density is 
    \\* \( {\displaystyle{ \frac{\lambda^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\lambda x}, \quad x > 0 }} \) 
    \\* where $\Gamma(\alpha)$ is a number that makes density integrate to 1
  \end{tightcenter}

  \begin{itemize}
    \item density of $\chi_1^2$ RV $= \frac{1}{\sqrt{2\pi}} v^{-1/2} e^{-v/2}, \quad v > 0$
      \\* $\quad\quad\quad\quad\quad\quad\quad\; = Gamma(\frac{1}{2}, \frac{1}{2})$
    \item $\chi^2_n$ RV $\sim Gamma(\frac{n}{2}, \frac{1}{2})$
      \begin{itemize}
        \item $\chi^2_n$ is a special case of Gamma!
      \end{itemize}
    \item if $X_1 \sim Gamma(\alpha_1, \lambda)$ and $X_2 \sim Gamma(\alpha_2, \lambda)$ are independent, then $X_1 + X_2 \sim Gamma(\alpha_1 + \alpha_2, \lambda)$
  \end{itemize}

  \subsection{t distribution}

  let $Z \sim N(0, 1)$ and $V \sim \chi^2_n$ be independent. 

  \begin{tightcenter}
    $ t_n = \frac{Z}{\sqrt{V/n}} $
    \\* has a $t$ distribution with $n$ degrees of freedom.
  \end{tightcenter}

  \begin{itemize}
    \item $t$ distribution is symmetric around 0
    \item $n \to \infty$, $t_n \to Z$
  \end{itemize}

  \subsection{F distribution}

  let $V \sim \chi^2_m$ and $W \sim \chi^2_n$ be independent. 

  \begin{tightcenter}
    $F_{m, n} = \frac{V/m}{W/n}$ 
    \\* has an $F$ distribution with $(m,n)$ degrees of freedom.
  \end{tightcenter}

  \begin{itemize}
    \item even if $m, n$, still two r.v.s as they are independent
    \item for $T \sim t_n$, $T^2 = \frac{Z^2}{V/n} \sim F_{1,n}$
  \end{itemize}

  \subsection{i.i.d. random variables}

  let $X_1, \dots, X_n$ be iid RVs with mean $\bar{X}$. 

  \begin{tightcenter}
    sample variance, $S^2 = \frac{1}{n-1} \sum^n_{i=1} (X_i - \bar{X})^2$
  \end{tightcenter}

  let $X_1, \dots, X_n$ be iid $N(\mu, \sigma^2)$. $\bar{X} = \frac{1}{n} \sum^n_{i=1} X_i$. 
  \begin{itemize}
    \item $\bar{X} \sim N(\mu, \frac{\sigma^2}{n})$
    \item $E(\bar{X}) = \mu$, $\quad\var(\bar{X}) = \frac{\sigma^2}{n}$
    \item $\frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0,1)$
    \item $\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}$
      \begin{itemize}
        \item proof: $\sum^n_{i=1}(\frac{X_i - \mu}{\sigma})^2 = \frac{1}{\sigma^2} \sum^n_{i=1} (X_i-\bar{X})^2 + n(\frac{\bar{X} - \mu}{\sigma})^2$
        \item LHS $\sim\chi^2_n$ by definition
        \item rightmost term $\sim\chi^2_1$
      \end{itemize}
    \item $\bar{X}$ and $S^2$ are independent
    \item $S$ is an estimate of $\sigma$
    \item $\frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}$
  \end{itemize}

  \subsection{Multivariate normal distribution}

  let $\mathbf{\mu}$ be a $k \times 1$ vector and $\mathbf{\Sigma}$ be a positive-definite symmetric $k \times k$ matrix.

  \begin{tightcenter}
    the random vector $\mathbf{X} = (X_1, \dots, X_k)'$ has a multivariate normal distribution $N(\mathbf{\mu}, \mathbf{\Sigma})$ if its density function is 

    \( {\displaystyle{ \frac{1}{(2\pi)^{k/2} \sqrt{det\mathbf{\Sigma}}} \exp \left(-\frac{(\mathbf{x} - \mathbf{\mu})' \mathbf{\Sigma}^{-1}(\mathbf{x} - \mathbf{\mu})}{2}\right) }} \) 
  \end{tightcenter}

  \begin{itemize}
    \item $E(\mathbf{X}) = \mathbf{\mu}, \quad \var(\mathbf{X}) = \mathbf{\Sigma}$
    \item for any non-zero $k \times 1 $ vector $\mathbf{a}$, 
      $\mathbf{a'X} \sim N(\mathbf{a'\mu}, \mathbf{a'\Sigma a})$
      \begin{itemize}
        \item $\mathbf{a}'\mathbf{\Sigma a} > 0$ because $\mathbf{\Sigma}$ is positive-definite
        \item $m=1$
      \end{itemize}
    \item two multinomial normal random vectors $\mathbf{X}_1$ and $\mathbf{X}_2$, sizes $h$ and $k$, are independent if $\cov (\mathbf{X}_1, \mathbf{X}_2) = 0_{h \times k}$
  \end{itemize}

  \section{03. POINT ESTIMATION}

  for a variable $v$ in population $N$, 
  \begin{tightcenter}
    \( {\displaystyle{ 
        \mu = \frac{1}{N} \sum^N_{i=1} v_i
        \quad \quad 
        \sigma^2 = \frac{1}{N} \sum^N_{i=1} (v_i - \mu)^2
    }} \) 
  \end{tightcenter}

  \begin{itemize}
    \item $\mu, \;\sigma^2$ are \ildefinition{parameters} (unknown constants)
    \item a \textbf{simple random sample} is used to estimate parameters: individuals drawn from the population at random without replacement
  \end{itemize}

  \subsubsection{binary variable}

  for variable $v$ with proportion $p$ in the population,

  \begin{tightcenter}
    $\mu = p, \quad\quad \sigma^2 = p(1-p)$
  \end{tightcenter}

  \subsubsection{single random draw}

  for variable $v$ (population of size $N$, mean $\mu$, variance $\sigma^2$),
  let $X$ be the chosen $v$-value.

  \begin{tightcenter}
    $E(X) = \mu$, $\quad\quad \var(X) = \sigma^2$
  \end{tightcenter}

  \subsubsection{draws with replacement}

  let $X_1, \dots, X_n$ be random draws with replacement from a population of mean $\mu$ and variance $\sigma^2$.

  \begin{tightcenter}
    random sample mean, \( {\displaystyle{ \bar{X} = \frac{1}{n}\sum^n_{i=1}X_i }} \) 

    $X_1, \dots, X_n$ are iid with $E(X_i) = \mu$, $\var(X_i) = \sigma^2$

    $E(\bar{X}) = TODO$, $\var(\bar{X}) = TODO$
  \end{tightcenter}

  let $x_1, \dots, x_n$ be realisations of $n$ random draws with replacement from the population.

  \begin{tightcenter}
    sample mean, \( {\displaystyle{ \bar{x} = \frac{1}{n} \sum^n_{i=1} x_i }} \) 

  \end{tightcenter}

  \begin{itemize}
    \item as $n \to \infty$, $\bar{x} \to \mu$  (LLN)
    \item sample distribution, $x_i$ has the same distribution as $X_i$ and the population distribution 
  \end{itemize}

  \subsubsection{representativeness}

  \begin{itemize}
    \item $X_1, \dots, X_n$ is \textbf{representative} of the population
      \begin{itemize}
        \item as $n$ gets larger, $\bar{X}$ gets closer to $\mu$
      \end{itemize}
    \item $x_1, \dots, x_n$ are \textit{likely} representative of the population
  \end{itemize}

  \subsection{estimating mean}

  given data $x_1, \dots, x_n$, 

  \begin{itemize}
    \item sample mean, $ \bar{x} = \frac{1}{n} \sum^n_{i=1} x_i $ is an  \ildefinition{estimate} of $\mu$
    \item the error in $\bar{x}$ is $\mu - \bar{x}$; $\quad$ it cannot be estimated
    \item $\bar{x}$ is a realisation of the \textbf{estimator} $\bar{X}$
      \begin{itemize}
        \item this realisation is used to estimate $\mu$
      \end{itemize}
  \end{itemize}

  \subsubsection{standard error}

  the size of error in estimate $\bar{x}$ is roughly \( {\displaystyle{ SD(\bar{X}) = \frac{\sigma}{\sqrt{n}} }} \)   

  the \textbf{standard error} (SE) in $\bar{x}$ is $\frac{\sigma}{\sqrt{n}}$

  \begin{itemize}
    \item SE is a constant by definition: $SE = SD(\hat{X}) = \frac{\sigma}{\sqrt{n}}$
  \end{itemize}

  \subsubsection{estimating $\sigma$}

  intuitive estimate of $\sigma^2$, $\hat{\sigma}^2 = \frac{1}{n} \sum^n_{i=1} (x_i-\bar{x})^2 $

  \begin{tightcenter}
    sample variance, \( {\displaystyle{ s^2 = \frac{1}{n-1} \sum^n_{i=1} (x_i-\bar{x})^2 }} \) 

    $E(s^2) = \sigma^2$
  \end{tightcenter}






\end{multicols*}

\end{document}
