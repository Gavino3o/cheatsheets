\documentclass[10pt, landscape]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{calc}
\usepackage{multicol}
\usepackage[a4paper,margin=3mm,landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{bm}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\usepackage{newtxtext} 
\usepackage{enumitem}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\setlist{nosep}
% for including images
\graphicspath{ {./images/} }

\pdfinfo{
  /Title (ST2132.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Jovyn Tan)
  /Subject (ST2132)
/Keywords (ST2132, nus,cheatsheet,pdf)}

% Turn off header and footer
\pagestyle{empty}

% redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
  {-1ex plus -.5ex minus -.2ex}%
  {0.5ex plus .2ex}%x
{\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
  {-1explus -.5ex minus -.2ex}%
  {0.5ex plus .2ex}%
{\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
  {-1ex plus -.5ex minus -.2ex}%
  {1ex plus .2ex}%
{\normalfont\small\bfseries}}%
\makeatother

\renewcommand{\familydefault}{\sfdefault}
\renewcommand\rmdefault{\sfdefault}
%  makes nested numbering (e.g. 1.1.1, 1.1.2, etc)
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\renewcommand\labelitemii{•}
\renewcommand\labelitemiii{•}

\definecolor{mathblue}{cmyk}{1,.72,0,.38}
\everymath\expandafter{\the\everymath \color{mathblue}}

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
%% adjust spacing for all itemize/enumerate
\setlength{\leftmargini}{0.5cm}
\setlength{\leftmarginii}{0.5cm}
\setlist[itemize,1]{leftmargin=2mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,2]{leftmargin=4mm,labelindent=1mm,labelsep=1mm}

\newcommand{\cov}{\mathop{\mathrm{cov}}}
\newcommand{\var}{\mathop{\mathrm{var}}}
\newcommand{\xbar}{\bar{x}}
\newcommand{\Xbar}{\bar{X}}

\DeclareRobustCommand{\svdots}{% s for `scaling'
  \vbox{%
    \baselineskip=0.33333\normalbaselineskip
    \lineskiplimit=0pt
    \hbox{.}\hbox{.}\hbox{.}%
    \kern-0.2\baselineskip
  }%
}

% adding my commands
\input{../commands/style-helpers.tex}
\input{../commands/math.tex}

% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols*}{4}
  % multicol parameters
  \setlength{\columnseprule}{0.25pt}

  \begin{center}
    \fbox{%
      \parbox{0.8\linewidth}{\centering \textcolor{black}{
          {\Large\textbf{ST2132}}
        \\ \normalsize{AY23/24 SEM 1}}
        \\ {\footnotesize \textcolor{gray}{github/jovyntls}}
      }%
    }
  \end{center}

  \section{01. PROBABILITY}

  \begin{itemize}
    \item \definition[of an event]{probability} the limiting relative frequency of its occurrence as the experiment is repeated many times
    \item the \textbf{realisation} $x$ is a constant, and $X$ is a generator
      \begin{itemize}
        \item running $r$ experiments gives us $r$ realisations $x_1, \dots, x_r$
      \end{itemize}
  \end{itemize}

  \subsection{Expectation}

  \begin{tightcenter}
    \begin{multicols*}{2}
      \textbf{discrete}: \\* (mass function) \\* \( {\displaystyle{ E(X) := \sum^n_{i=1}x_ip_i }} \) 

      \textbf{continuous}: \\* (density function) \\* \( {\displaystyle{ E(X) := \int^\infty_{-\infty} xf(x) \dx }} \) 
    \end{multicols*}
  \end{tightcenter}

  \subsubsection{expectation of a function $h(X)$}

  $ E\{h(X)\} = \begin{cases}
        \sum^n_{i=1} h(x_i)p_i &\text{$X$ is discrete} \\ 
        \int^\infty_{-\infty} h(x)f(x)\dx &\text{$X$ is continuous} \\ 
      \end{cases}$ 

  \subsection{Variance}

  \begin{tightcenter}
      \textbf{variance}, $\var(X) := E\{(X-\mu)^2\}$

    \textbf{standard deviation}, $SD(X) := \sqrt{\var(X)}$
  \end{tightcenter}

  \begin{itemize}
    \item $\var(X) = E(X^2) - E(X)^2$
    \item $E(X - \mu) = 0$
  \end{itemize}

  \subsection{Law of Large Numbers}

  mean and variance of $r$ realisations:  

  \begin{tightcenter}
    \( {\displaystyle{ \xbar := \frac{1}{r} \sum^r_{i=1} x_i }} \) 
    $\quad\quad$
    \( {\displaystyle{ v := \frac{1}{r}\sum^r_{i=1} (x_i - \xbar)^2 }} \) 
  \end{tightcenter}

  \textbf{LLN:} for a function $h$, as $r \to \infty$, 

  \begin{tightcenter}
    \( {\displaystyle{ \frac{1}{r} \sum^r_{i=1}h(x_i) \to E\{h(X)\}  }} \) 

    $\xbar \to E(X)$, $\quad v \to \var(X)$
  \end{tightcenter}

  \subsubsection{Monte Carlo approximation}

  simulate $x_1, \dots, x_r$ from $X$. by LLN, as $r \to \infty$, the approximation becomes exact

  \begin{tightcenter}
    \( {\displaystyle{ E\{h(X)\} \approx \frac{1}{r} \sum^r_{i=1} h(x_i) }} \) 
  \end{tightcenter}

  \subsection{Joint Distribution}

  (\textbf{discrete}) mass function:
  \begin{tightcenter}
    \( {\displaystyle{P( X = x_i, Y = y_j ) = p_{ij}}} \)  
  \end{tightcenter}

  (\textbf{continuous}) density function:
  \begin{tightcenter}
    \( {\displaystyle{f : \mathbb{R}^2  \to [0, \infty), \int^\infty_{-\infty} f(x, y) \dx \dy = 1}} \) 
  \end{tightcenter}

  (\textbf{expectation}) for $h : \mathbb{R}^2 \to \mathbb{R}$,
  \begin{tightcenter}
    \( {\displaystyle{ E\{ h(X,Y)\} = }} \) 
    \( {\displaystyle{ 
        \begin{cases}
          \sum^I_{i=1}\sum^J_{j=1} h(x_i, y_j) p_{ij} & X \text{ is discrete}
          \\ \int^\infty_{-\infty}\int^\infty_{-\infty} h(x, y) f(x, y) \dx \dy & Y \text{ is continuous}
        \end{cases}
    }} \) 
  \end{tightcenter}

  \subsection{Algebra of RV's}

  let $X, Y$ be RVs and $a, b, c$ be constants

  \begin{itemize}
    \item $Z = aX + bY + c$ is also an RV
      \begin{itemize}
        \item $z = ax + by + c$ is a realisation of $Z$
      \end{itemize}
    \item linearity of expectation: $E(Z) = aE(X) + bE(Y) + c$
    \item any theorem about a RV is true about a constant 
  \end{itemize}

  \subsection{Covariance}

  let $\mu_X = E(X)$, $\mu_Y=E(Y)$.

  \begin{tightcenter}
    \textbf{covariance}, $\cov(X,Y) = E\{ (X-\mu_X)(Y-\mu_Y) \}$
  \end{tightcenter}

  \begin{itemize}
    \item $\cov(X, Y) = E(XY) - \mu_X\mu_Y$
    \item $\cov(X, Y) = \cov(Y,X)$
    \item $\cov(X, X) = \var(X)$
    \item $\cov(W, aX+bY+c) = a\cov(W, X) + b\cov(W, Y)$
    \item $\var(aX + bY + c) = a^2 \var(X) + b^2 \var(Y) + 2ab\cov(X, Y)$
  \end{itemize}

  \subsection{joint $=$ marginal $\times$ conditional distributions}

  \begin{align*}
    f(x, y) &= f_X(x)f_Y(y \vert x) 
         \\ &= f_Y(y) f_X(x \vert y),  \quad x, y \in \mathbb{R}
  \end{align*}

  \begin{itemize}
    \item $f(x, y)$ is the \textit{joint density}
    \item $f_X(x), f_Y(y)$ are the \textit{marginal densities}
    \item $f_Y(\cdot \vert x)$ is the \textbf{conditional} density of $Y$ given $X=x$
    \item $f_X(\cdot \vert y)$ is the \textbf{conditional} density of $X$ given $Y=y$ 
    \item for discrete case, \textit{density} $\equiv$ \textit{probability}, $x \equiv x_i$, $y \equiv y_j$
  \end{itemize}


  \subsection{Independence}

  \begin{itemize}
    \item $X, Y$ are independent $\iff \forall x, y \in \mathbb{R}$, 

      \begin{enumerate}
        \item $f(x, y) = f_X(x)f_Y(y)$ 
        \item $f_Y(y\vert x) = f_Y(y)$
        \item $f_X(x\vert y) = f_Y(x)$
      \end{enumerate}

    \item $X, Y$ are independent  $\Rightarrow$
      \begin{itemize}
        \item $E(XY) = E(X)E(Y)$ 
        \item $\cov(X,Y) = 0$
      \end{itemize}
      (the converse does not hold)
  \end{itemize}

  \subsection{Conditional expectation}

  \subsubsection{discrete case}

  let $f_Y(\cdot \vert x_i)$ be the conditional pmf of $Y$ given $X = x_i$.

  \begin{tightcenter}
    \( {\displaystyle{ E[Y \vert x_i] := \sum^J_{j=1} y_j f_Y (y_j \vert x_i) }} \) 

    \( {\displaystyle{ \var[Y\vert x_i] := \sum^J_{j=1} (y_j - E[Y\vert x_i])^2 f_Y (y_j \vert x_i) }} \) 
  \end{tightcenter}

  $E[Y\vert x_i]$ is like $E(Y)$, with conditional distribution replacing marginal distribution  $f_Y(\cdot)$. 
  likewise, $\var[Y\vert x_i]$ like $\var(Y)$.

  \subsubsection{continuous case}

  \begin{tightcenter}
    \( {\displaystyle{ E[Y \vert x] := \int^\infty_{-\infty} y f_Y (y \vert x) \dy }} \) 
  \end{tightcenter}

  \begin{align*}
    \var[Y\vert x] &:= \int^\infty_{-\infty} (y - E[Y\vert x])^2 f_Y (y \vert x) \dy 
                \\ &= E(Y^2 \vert x) - \{E(Y \vert x)\}^2
  \end{align*}

  \subsection{Distributions}

  if $X$ is iid with expectation $\mu$, SD $\sigma$ and $S_n = \sum^n_{i=1}X_i$,

  \begin{itemize}
    \item  $ E(S_n) = n\mu$
    \item $SD(S_n) = \sqrt n \sigma $
    \item variance of sum = sum of variances
      $ \var(\sum^n_{i=1} X_i) = \sum^n_{i=1} \var(x_i) $ 
  \end{itemize}

  \subsubsection{bernoulli}

  $X \sim Bernoulli(p) \quad \Rightarrow$ coin flip with probability $p$

  \begin{tightcenter}
    $E(X_i) = p \quad\quad \var(X_i) = p(1-p)$
    $E(S_n) = np \quad\quad \var(S_n) = np(1-p)$
  \end{tightcenter}

  \subsubsection{binomial}

  $X \sim Bin(n, p) \quad \Rightarrow$ 
  $X_i \mathop{\sim}\limits^{i.i.d.} Bernoulli(p)$

  \begin{tightcenter}
    $E(X) = np, \quad \var(X) = np(1-p)$

    \( {\displaystyle{ E(X) = \sum^n_{k=1} k \binom{n}{k} p^k (1-p)^{n-k} }} \)  
  \end{tightcenter}

  \subsubsection{multinomial}

  $X \sim Multinomial(n, \mathbf{p})$ 

  \begin{itemize}
    \item for $k$ outcomes $E_1, \dots, E_k$, $\; Pr(E_i) = p_i$.
      For some $1 \leq i \leq k$, $E_i$ occurs $X_i$ times in $n$ runs.
  \end{itemize}

  $(X_1, \dots, X_k)$ has the \textbf{multinomial distribution:}

  \begin{tightcenter}
    \( {\displaystyle{ Pr(X_1 = x_1, \dots, X_k = x_k) = \binom{n}{x_1 \dots x_k} \Pi^k_{i=1} p_i^{x_i} }} \)   
  \end{tightcenter}

  \begin{itemize}
    \item where $\binom{n}{x_1 \dots x_k} = \frac{n!}{x_1! x_2! \dots x_k!}$
      \begin{itemize}
        \item combinatorially, \# of arrangements of $x_1, \dots, x_k$
        \item $\sum^n_{i=1}x_i = n, \quad x_i \ge 0$
      \end{itemize}
  \end{itemize}

  \begin{tightcenter}
    $E(X) = \left[\begin{smallmatrix}np_1 \\ np_2 \\ \svdots \\ np_k\end{smallmatrix}\right]$,  
    $\quad \var(X_i) = np_i (1-p_i)$

    $\var(X) =$ \textit{covariance matrix} $M$ with $m_{ij} = \begin{cases}
      \var(X_i) & \text{if }i=j
      \\ \cov(X_i, X_j) & \text{if }i \neq j
    \end{cases} $
  \end{tightcenter}

  \begin{itemize}
    \item $\cov(X_i, X_j) < 0$
    \item $X_i \sim Bin(n, p_i)$
    \item $X_i + X_j \sim Bin(n, p_i + p_j)$
  \end{itemize}


  \section{02. PROBABILITY (2)}

  \subsection{Mean Square Error (MSE)}

  $$MSE = E\{ (Y-c)^2 \}$$

  \begin{itemize}
    \item predicting Y:
      \\* $MSE = \var(Y) + \{ E(Y) - c\}^2$
      \begin{itemize}
        \item $\min MSE=\var(Y)$ when $c = E(Y)$
      \end{itemize}
    \item $Y$ and $X$ are correlated: 
      \\* $MSE = \var[Y \vert x] + \{ E[Y\vert x] - c \}^2$
      \\* $MSE = E[(Y-c)^2 \vert x] = E[\{ Y - E(Y) \}^2 \vert x]$
      \begin{itemize}
        \item $\min MSE = \var(Y \vert x)$ when $c = E[Y \vert x]$
        \item if $c = E(Y)$ instead of $E(Y \vert x)$ $\Rightarrow$ the MSE increases by $( E(Y \vert x) - E(Y) )^2$
      \end{itemize}
  \end{itemize}

  \subsubsection{mean MSE}

  $$\frac{1}{n} \sum^n_{i=1}\var[Y \vert x_i] \approx E\{ \var[Y \vert X] \}$$

  \subsubsection{random conditional expectations}

  let $X, Y$ be r.v.s.
  \begin{itemize}
    \item $E[Y \vert X]$ is a r.v. which takes value $E[Y \vert x]$ with probability/density $f_X(x)$
    \item $\var[Y \vert X]$ is a r.v. which takes value $\var[Y \vert x]$ with probability/density $f_X(x)$ 
  \end{itemize}

  \begin{tightcenter}
    $E(E[X_2 \vert X_1]) = E(X_2)$

    $\var(E[X_2 \vert X_1]) + E(\var[X_2 \vert X_1]) = \var(X_2)$
  \end{tightcenter}

  \subsection{CDF (cumulative distribution function)}

  for r.v. $X$, let $F(x) = P(X \leq x)$

  \begin{itemize}
    \item domain: $\mathbb{R}$; $\;$ codomain: $[0,1]$
  \end{itemize}

  \begin{tightcenter}
    $F(x) = \int^\infty_{-\infty} f(x) \dx$
  \end{tightcenter}

  \subsection{Standard Normal Distribution}

  \begin{tightcenter}
    $Z \sim N(0, 1)$ has density function \\* 
    \( {\displaystyle{ \phi(z) = \frac{1}{\sqrt{2\pi}} \exp \{ -\frac{z^2}{2} \}, \quad -\infty < z < \infty }} \) 
  \end{tightcenter}

  $$E(Z) = 0, \quad \var(Z) = 1$$

  \begin{tightcenter}
    \textbf{CDF}, $\Phi(x) = P(Z \leq x) = \int^x_{-\infty} \phi(z) \mathop{dz}$
  \end{tightcenter}

  \begin{itemize}
    \item  $E(Z) = \int^\infty_{-\infty} z\phi(z) \mathop{dz} = 0$
      \begin{itemize}
        \item  $E(Z^2) = \int^\infty_{-\infty} z^2\phi(z) \mathop{dz} = 1$
          \item $E(Z^{2k+1}) = 0 \quad \forall k \in \mathbb{Z}_{\ge 0}$
      \end{itemize}
  \end{itemize}

  \subsubsection{general normal distribution}

  let  $X \sim N(\mu, \sigma^2)$ and $Y \sim N(\nu, \tau^2)$

  \begin{tightcenter}
    \textbf{standardisation:} $\frac{X - \mu}{\sigma} \sim N(0, 1)$
  \end{tightcenter}

  \begin{itemize}
    \item summations: 
      \begin{itemize}
        \item for constants $a, b \neq 0$, 
          \\* $\quad \; a + bX \sim N(a+b\mu, \; b^2\sigma^2)$
        \item $X + Y \sim N(\mu + \nu, \; \sigma^2 + \tau^2 + 2\cov(X,Y))$
          \begin{itemize}
            \item $\cov(X, Y) = 0$, $\quad \Rightarrow \quad$ $X \perp Y$
            \item $X \perp Y$ $\quad \Rightarrow \quad$ $X + Y \sim N(\mu + \nu, \; \sigma^2 + \tau^2)$
          \end{itemize}
      \end{itemize}

    \item for $W = a + bX$, 
      \begin{itemize}
        \item density,  $f_W(w) = \frac{d}{dw} F_W(w)$
        \item CDF, $\;\;\; F_W(w) = P(X \leq \frac{w-a}{b}) = \Phi(\frac{w-a}{b})$
      \end{itemize}
  \end{itemize}

  \subsection{Central Limit Theorem}

  let $X_1, \dots, X_n$ be iid rv's with expectation $\mu$ and SD $\sigma$, with $ S_n = \sum^n_{i=1} X_i $

  \begin{tightcenter}
    \textbf{CLT} 
    \\* as $n \to \infty$, the distribution of the standardised $S_n = \frac{S_n - n\mu}{\sqrt{n}\sigma}$ converges to $N(0,1)$
  \end{tightcenter}

  \begin{itemize}
    \item $E(S_n) = n\mu$, $\; \var(S_n) = n\sigma^2$
    \item for large $n$, approximately $S_n \sim N(n\mu, n\sigma^2)$
  \end{itemize}

  \subsubsection{bernoulli}

  let $X_i \sim Bernoulli(p)$. then $S_n \sim Binom(n, p)$
  \begin{itemize}
    \item for large $n$, $S_n = N(np, np(1-p))$
    \item CLT: standardised $\frac{S_n - np}{\sqrt{n}\sqrt{p(1-p)}} \to N(0,1)$ as $n \to \infty$
  \end{itemize}

  \subsection{Distributions}

  \subsubsection{chi-square ($\chi^2$)}

  let $Z \sim N(0,1)$. 
  $\quad\Rightarrow$ then $Z^2 \sim \chi^2_1$

  \begin{itemize}
    \item $Z^2$ has $\chi^2$ distribution with 1 degree of freedom
    \item degrees of freedom = number of RVs in the sum
  \end{itemize}

  \begin{tightcenter}
    $E(Z^2) = 1, \quad E(Z^4) = 3$
    \\* $\var(Z^2) = E(Z^4) - \{ E(Z^2) \}^2 = 2$
  \end{tightcenter}

  let $V_1, \dots, V_n$ be iid $\chi^2_1$ RVs and $V = \sum^n_{i=1}V_i$. then 
  \begin{tightcenter}
    $V \sim \chi^2_n$

    $E(V) = n \quad \var(V) = 2n$
  \end{tightcenter}

  \subsubsection{gamma}

  \begin{tightcenter}
    let $\alpha, \lambda > 0$. The $Gamma(\alpha, \lambda)$ density is 
    \\* \( {\displaystyle{ \frac{\lambda^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\lambda x}, \quad x > 0 }} \) 
    \\* where $\Gamma(\alpha)$ is a number that makes density integrate to 1
  \end{tightcenter}

  \begin{itemize}
    \item $\chi^2_n$ RV $\sim Gamma(\frac{n}{2}, \frac{1}{2})$
      \begin{itemize}
        \item $\chi^2_n$ is a special case of Gamma!
        \item density of $\chi_1^2$ RV $= \frac{1}{\sqrt{2\pi}} v^{-1/2} e^{-v/2}, \quad v > 0$
          \\* $\quad\quad\quad\quad\quad\quad\quad\;\; = Gamma(\frac{1}{2}, \frac{1}{2})$
      \end{itemize}
    \item if $X_1 \sim Gamma(\alpha_1, \lambda)$ and $X_2 \sim Gamma(\alpha_2, \lambda)$ are independent, then $X_1 + X_2 \sim Gamma(\alpha_1 + \alpha_2, \lambda)$
  \end{itemize}

  \subsubsection{t distribution}

  let $Z \sim N(0, 1)$ and $V \sim \chi^2_n$ be independent. 

  \begin{tightcenter}
    $\frac{Z}{\sqrt{V/n}} \sim t_n $
    \\* has a $t$ distribution with $n$ degrees of freedom.
  \end{tightcenter}

  \begin{itemize}
    \item $t$ distribution is symmetric around 0
    \item $t_n \to Z \;$ as $\; n \to \infty$ (because $\frac{V}{n} \to 1$)
  \end{itemize}

  \subsubsection{\textit{F} distribution}

  let $V \sim \chi^2_m$ and $W \sim \chi^2_n$ be independent. 

  \begin{tightcenter}
    $\frac{V/m}{W/n} \sim F_{m,n}$ 
    \\* has an $F$ distribution with $(m,n)$ degrees of freedom.
  \end{tightcenter}

  \begin{itemize}
    \item even if $m=n$, still two RVs $V,W$ as they are independent
    \item for $T \sim t_n$, $T^2 = \frac{Z^2}{V/n} \sim F_{1,n}$
  \end{itemize}

  \subsection{IID Random Variables}

  let $X_1, \dots, X_n$ be iid RVs with mean $\bar{X}$. 

  \begin{tightcenter}
    \textbf{sample variance}, \( {\displaystyle{ S^2 = \frac{1}{n-1} \sum^n_{i=1} (X_i - \bar{X})^2 }} \) 

    $S$ is an estimate of $\sigma$
  \end{tightcenter}

  let $X_1, \dots, X_n$ be iid $N(\mu, \sigma^2)$. $\bar{X} = \frac{1}{n} \sum^n_{i=1} X_i$. 
  \begin{tightcenter}
    $\bar{X} \sim N(\mu, \frac{\sigma^2}{n})$

    $E(\bar{X}) = \mu$, $\quad\var(\bar{X}) = \frac{\sigma^2}{n}$
  \end{tightcenter}

  more distributions:
  \begin{tightcenter}
    $\frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0,1)$

    $\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}$

    $\frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}$
  \end{tightcenter}

  \begin{itemize}
    \item $\bar{X}$ and $S^2$ are independent
  \end{itemize}

  \subsection{Multivariate Normal Distribution}

  let $\bm{\mu}$ be a $k \times 1$ vector and $\bm{\Sigma}$ be a \textit{positive-definite} symmetric $k \times k$ matrix.

  \begin{tightcenter}
    the random vector $\bm{X} = (X_1, \dots, X_k)'$ has a multivariate normal distribution $N(\bm{\mu}, \bm{\Sigma})$ if its density function is 

    \( {\displaystyle{ \frac{1}{(2\pi)^{k/2} \sqrt{det\bm{\Sigma}}} \exp \left(-\frac{(\bm{x} - \bm{\mu})' \bm{\Sigma}^{-1}(\bm{x} - \bm{\mu})}{2}\right) }} \) 
  \end{tightcenter}

  \begin{itemize}
    \item $E(\bm{X}) = \bm{\mu}, \quad \var(\bm{X}) = \bm{\Sigma}$
    \item for any non-zero $k \times 1 $ vector $\bm{a}$, 
      \begin{tightcenter}
        $\bm{a'X} \sim N(\bm{a'\mu}, \bm{a'\Sigma a})$
      \end{tightcenter}
      \begin{itemize}
        \item $\bm{a}'\bm{\Sigma a} > 0$ because $\bm{\Sigma}$ is positive-definite
        \item the product $\bm{a'X}$ is a scalar (same for $\bm{a'\mu}, \bm{a'\Sigma a}$)
      \end{itemize}
    \item two multinomial normal random vectors $\bm{X}_1$ and $\bm{X}_2$, sizes $h$ and $k$, are independent if $\cov (\bm{X}_1, \bm{X}_2) = \bm{0}_{h \times k}$
      \begin{itemize}
        \item $(X_1-\Xbar, \dots, X_n - \Xbar)$ has a multivariate normal distribution; $\;$
          the covariance between $\Xbar$ and $(X_1-\Xbar, \dots, X_n - \Xbar)$ is $0$, thus they are independent
      \end{itemize}
  \end{itemize}

  \section{03. POINT ESTIMATION}

  for a variable $v$ in population $N$, 
  \begin{tightcenter}
    \( {\displaystyle{ 
        \mu = \frac{1}{N} \sum^N_{i=1} v_i
        \quad \quad 
        \sigma^2 = \frac{1}{N} \sum^N_{i=1} (v_i - \mu)^2
    }} \) 
  \end{tightcenter}

  \begin{itemize}
    \item $\mu, \;\sigma^2$ are \ildefinition{parameters} (unknown constants)
    \item a \textbf{simple random sample} is used to estimate parameters: individuals drawn from the population at random without replacement
  \end{itemize}

  \subsubsection{binary variable}

  for variable $v$ with proportion $p$ in the population,

  \begin{tightcenter}
    $\mu = p, \quad\quad \sigma^2 = p(1-p)$
  \end{tightcenter}

  \subsubsection{single random draw}

  for variable $v$ (population of size $N$, mean $\mu$, variance $\sigma^2$),
  let $X$ be the chosen $v$-value.

  \begin{tightcenter}
    $E(X) = \mu$, $\quad\quad \var(X) = \sigma^2$
  \end{tightcenter}

  \subsubsection{draws with replacement}

  let $X_1, \dots, X_n$ be random draws with replacement from a population of mean $\mu$ and variance $\sigma^2$.

  \begin{tightcenter}
    random sample mean, \( {\displaystyle{ \Xbar = \frac{1}{n}\sum^n_{i=1}X_i }} \) 

    $X_1, \dots, X_n$ are iid with $E(X_i) = \mu$, $\var(X_i) = \sigma^2$

    $E(\Xbar) = TODO$, $\var(\Xbar) = TODO$
  \end{tightcenter}

  let $x_1, \dots, x_n$ be realisations of $n$ random draws with replacement from the population.

  \begin{tightcenter}
    sample mean, \( {\displaystyle{ \xbar = \frac{1}{n} \sum^n_{i=1} x_i }} \) 

  \end{tightcenter}

  \begin{itemize}
    \item as $n \to \infty$, $\xbar \to \mu$  (LLN)
    \item sample distribution, $x_i$ has the same distribution as $X_i$ and the population distribution 
  \end{itemize}

  \subsubsection{representativeness}

  \begin{itemize}
    \item $X_1, \dots, X_n$ is \textbf{representative} of the population
      \begin{itemize}
        \item as $n$ gets larger, $\Xbar$ gets closer to $\mu$
      \end{itemize}
    \item $x_1, \dots, x_n$ are \textit{likely} representative of the population
  \end{itemize}

  \subsection{estimating mean}

  given data $x_1, \dots, x_n$, 

  \begin{itemize}
    \item sample mean, $ \xbar = \frac{1}{n} \sum^n_{i=1} x_i $ is an  \ildefinition{estimate} of $\mu$
    \item the error in $\xbar$ is $\mu - \xbar$; $\quad$ it cannot be estimated
    \item $\xbar$ is a realisation of the \textbf{estimator} $\Xbar$
      \begin{itemize}
        \item this realisation is used to estimate $\mu$
      \end{itemize}
  \end{itemize}

  \subsubsection{standard error}

  the size of error in estimate $\xbar$ is roughly \( {\displaystyle{ SD(\Xbar) = \frac{\sigma}{\sqrt{n}} }} \)   

  the \textbf{standard error} (SE) in $\xbar$ is $\frac{\sigma}{\sqrt{n}}$

  \begin{itemize}
    \item SE is a constant by definition: $SE = SD(\hat{X}) = \frac{\sigma}{\sqrt{n}}$
  \end{itemize}

  \subsubsection{estimating $\sigma$}

  intuitive estimate of $\sigma^2$, $\hat{\sigma}^2 = \frac{1}{n} \sum^n_{i=1} (x_i-\xbar)^2 $

  \begin{tightcenter}
    sample variance, \( {\displaystyle{ s^2 = \frac{1}{n-1} \sum^n_{i=1} (x_i-\xbar)^2 }} \) 

    $E(s^2) = \sigma^2$
  \end{tightcenter}






\end{multicols*}

\end{document}
