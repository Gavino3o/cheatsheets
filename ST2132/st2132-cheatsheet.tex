\documentclass[10pt, landscape]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{calc}
\usepackage{multicol}
\usepackage[a4paper,margin=3mm,landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{bm}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\usepackage{newtxtext} 
\usepackage{enumitem}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\setlist{nosep}
% for including images
\graphicspath{ {./images/} }

\pdfinfo{
  /Title (ST2132.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Jovyn Tan)
  /Subject (ST2132)
/Keywords (ST2132, nus,cheatsheet,pdf)}

% Turn off header and footer
\pagestyle{empty}

% redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
  {-1ex plus -.5ex minus -.2ex}%
  {0.5ex plus .2ex}%x
{\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
  {-1explus -.5ex minus -.2ex}%
  {0.5ex plus .2ex}%
{\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
  {-1ex plus -.5ex minus -.2ex}%
  {1ex plus .2ex}%
{\normalfont\small\bfseries}}%
\makeatother

\renewcommand{\familydefault}{\sfdefault}
\renewcommand\rmdefault{\sfdefault}
%  makes nested numbering (e.g. 1.1.1, 1.1.2, etc)
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\renewcommand\labelitemii{•}
\renewcommand\labelitemiii{•}

\definecolor{mathblue}{cmyk}{1,.72,0,.38}
\everymath\expandafter{\the\everymath \color{mathblue}}

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
%% adjust spacing for all itemize/enumerate
\setlength{\leftmargini}{0.5cm}
\setlength{\leftmarginii}{0.5cm}
\setlist[itemize,1]{leftmargin=2mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,2]{leftmargin=4mm,labelindent=1mm,labelsep=1mm}

\newcommand{\cov}{\mathop{\mathrm{cov}}}
\newcommand{\var}{\mathop{\mathrm{var}}}
\newcommand{\xbar}{\bar{x}}
\newcommand{\Xbar}{\bar{X}}
\newcommand{\seq}[2][n]{#2_1, \dots, #2_{#1}}

\DeclareRobustCommand{\svdots}{% s for `scaling'
  \vbox{%
    \baselineskip=0.33333\normalbaselineskip
    \lineskiplimit=0pt
    \hbox{.}\hbox{.}\hbox{.}%
    \kern-0.2\baselineskip
  }%
}

% adding my commands
\input{../commands/style-helpers.tex}
\input{../commands/math.tex}

% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols*}{4}
  % multicol parameters
  \setlength{\columnseprule}{0.25pt}

  \begin{center}
    \fbox{%
      \parbox{0.8\linewidth}{\centering \textcolor{black}{
          {\Large\textbf{ST2132}}
        \\ \normalsize{AY23/24 SEM 1}}
        \\ {\footnotesize \textcolor{gray}{github/jovyntls}}
      }%
    }
  \end{center}

  \section{01. PROBABILITY}

  \begin{itemize}
    \item \definition[of an event]{probability} the limiting relative frequency of its occurrence as the experiment is repeated many times
    \item the \textbf{realisation} $x$ is a constant, and $X$ is a generator
      \begin{itemize}
        \item running $r$ experiments gives us $r$ realisations $\seq[r]{x}$
      \end{itemize}
  \end{itemize}

  \subsection{Expectation}

  \begin{tightcenter}
    \begin{multicols*}{2}
      \textbf{discrete}: \\* (mass function) \\* \( {\displaystyle{ E(X) := \sum^n_{i=1}x_ip_i }} \) 

      \textbf{continuous}: \\* (density function) \\* \( {\displaystyle{ E(X) := \int^\infty_{-\infty} xf(x) \dx }} \) 
    \end{multicols*}
  \end{tightcenter}

  \subsubsection{expectation of a function $h(X)$}

  $ E\{h(X)\} = \begin{cases}
        \sum^n_{i=1} h(x_i)p_i &\text{$X$ is discrete} \\ 
        \int^\infty_{-\infty} h(x)f(x)\dx &\text{$X$ is continuous} \\ 
      \end{cases}$ 

  \subsection{Variance}

  \begin{tightcenter}
      \textbf{variance}, $\var(X) := E\{(X-\mu)^2\}$

    \textbf{standard deviation}, $SD(X) := \sqrt{\var(X)}$
  \end{tightcenter}

  \begin{itemize}
    \item $\var(X) = E(X^2) - E(X)^2$
    \item $E(X - \mu) = 0$
  \end{itemize}

  \subsection{Law of Large Numbers}

  mean and variance of $r$ realisations:  

  \begin{tightcenter}
    \( {\displaystyle{ \xbar := \frac{1}{r} \sum^r_{i=1} x_i }} \) 
    $\qquad$
    \( {\displaystyle{ v := \frac{1}{r}\sum^r_{i=1} (x_i - \xbar)^2 }} \) 
  \end{tightcenter}

  \textbf{LLN:} for a function $h$, as $r \to \infty$, 

  \begin{tightcenter}
    \( {\displaystyle{ \frac{1}{r} \sum^r_{i=1}h(x_i) \to E\{h(X)\}  }} \) 

    $\xbar \to E(X)$, $\quad v \to \var(X)$
  \end{tightcenter}

  \subsubsection{Monte Carlo approximation}

  simulate $\seq[r]{x}$ from $X$. by LLN, as $r \to \infty$, the approximation becomes exact

  \begin{tightcenter}
    \( {\displaystyle{ E\{h(X)\} \approx \frac{1}{r} \sum^r_{i=1} h(x_i) }} \) 
  \end{tightcenter}

  \subsection{Joint Distribution}

  (\textbf{discrete}) mass function:
  \begin{tightcenter}
    \( {\displaystyle{P( X = x_i, Y = y_j ) = p_{ij}}} \)  
  \end{tightcenter}

  (\textbf{continuous}) density function:
  \begin{tightcenter}
    \( {\displaystyle{f : \mathbb{R}^2  \to [0, \infty), \int^\infty_{-\infty} f(x, y) \dx \dy = 1}} \) 
  \end{tightcenter}

  (\textbf{expectation}) for $h : \mathbb{R}^2 \to \mathbb{R}$,
  \begin{tightcenter}
    \( {\displaystyle{ E\{ h(X,Y)\} = }} \) 
    \( {\displaystyle{ 
        \begin{cases}
          \sum^I_{i=1}\sum^J_{j=1} h(x_i, y_j) p_{ij} & X \text{ is discrete}
          \\ \int^\infty_{-\infty}\int^\infty_{-\infty} h(x, y) f(x, y) \dx \dy & Y \text{ is continuous}
        \end{cases}
    }} \) 
  \end{tightcenter}

  \subsection{Algebra of RV's}

  let $X, Y$ be RVs and $a, b, c$ be constants

  \begin{itemize}
    \item $Z = aX + bY + c$ is also an RV
      \begin{itemize}
        \item $z = ax + by + c$ is a realisation of $Z$
      \end{itemize}
    \item linearity of expectation: $E(Z) = aE(X) + bE(Y) + c$
    \item any theorem about a RV is true about a constant 
  \end{itemize}

  \subsection{Covariance}

  let $\mu_X = E(X)$, $\mu_Y=E(Y)$.

  \begin{tightcenter}
    \textbf{covariance}, $\cov(X,Y) = E\{ (X-\mu_X)(Y-\mu_Y) \}$
  \end{tightcenter}

  \begin{itemize}
    \item $\cov(X, Y) = E(XY) - \mu_X\mu_Y$
    \item $\cov(X, Y) = \cov(Y,X)$
    \item $\cov(X, X) = \var(X)$
    \item $\cov(W, aX+bY+c) = a\cov(W, X) + b\cov(W, Y)$
    \item $\var(aX + bY + c) = a^2 \var(X) + b^2 \var(Y) + 2ab\cov(X, Y)$
    \item $\var(\sum^N_{i=1}a_iX_i) = \sum^N_{i=1}a_i^2\var(X_i) + 2\sum_{1 \leq i < j \leq N} a_ia_j \cov(X_i,X_j)$
  \end{itemize}

  \subsection{joint $=$ marginal $\times$ conditional distributions}

  \begin{align*}
    f(x, y) &= f_X(x)f_Y(y \vert x) 
         \\ &= f_Y(y) f_X(x \vert y),  \quad x, y \in \mathbb{R}
  \end{align*}

  \begin{itemize}
    \item $f(x, y)$ is the \textit{joint density}
    \item $f_X(x), f_Y(y)$ are the \textit{marginal densities}
    \item $f_Y(\cdot \vert x)$ is the \textbf{conditional} density of $Y$ given $X=x$
    \item $f_X(\cdot \vert y)$ is the \textbf{conditional} density of $X$ given $Y=y$ 
    \item for discrete case, \textit{density} $\equiv$ \textit{probability}, $x \equiv x_i$, $y \equiv y_j$
  \end{itemize}


  \subsection{Independence}

  \begin{itemize}
    \item $X, Y$ are independent $\iff \forall x, y \in \mathbb{R}$, 

      \begin{enumerate}
        \item $f(x, y) = f_X(x)f_Y(y)$ 
        \item $f_Y(y\vert x) = f_Y(y)$
        \item $f_X(x\vert y) = f_Y(x)$
      \end{enumerate}

    \item $X, Y$ are independent  $\Rightarrow$
      \begin{itemize}
        \item $E(XY) = E(X)E(Y)$ 
        \item $\cov(X,Y) = 0$
      \end{itemize}
      (the converse does not hold)
  \end{itemize}

  \subsection{Conditional expectation}

  \subsubsection{discrete case}

  let $f_Y(\cdot \vert x_i)$ be the conditional pmf of $Y$ given $X = x_i$.

  \begin{tightcenter}
    \( {\displaystyle{ E[Y \vert x_i] := \sum^J_{j=1} y_j f_Y (y_j \vert x_i) }} \) 

    \( {\displaystyle{ \var[Y\vert x_i] := \sum^J_{j=1} (y_j - E[Y\vert x_i])^2 f_Y (y_j \vert x_i) }} \) 
  \end{tightcenter}

  $E[Y\vert x_i]$ is like $E(Y)$, with conditional distribution replacing marginal distribution  $f_Y(\cdot)$. 
  likewise, $\var[Y\vert x_i]$ like $\var(Y)$.

  \subsubsection{continuous case}

  \begin{tightcenter}
    \( {\displaystyle{ E[Y \vert x] := \int^\infty_{-\infty} y f_Y (y \vert x) \dy }} \) 
  \end{tightcenter}

  \begin{align*}
    \var[Y\vert x] &:= \int^\infty_{-\infty} (y - E[Y\vert x])^2 f_Y (y \vert x) \dy 
                \\ &= E(Y^2 \vert x) - \{E(Y \vert x)\}^2
  \end{align*}

  \subsection{Distributions}

  if $X$ is iid with expectation $\mu$, SD $\sigma$ and $S_n = \sum^n_{i=1}X_i$,

  \begin{itemize}
    \item  $ E(S_n) = n\mu$
    \item $SD(S_n) = \sqrt n \sigma $
    \item variance of sum = sum of variances
      $ \var(\sum^n_{i=1} X_i) = \sum^n_{i=1} \var(x_i) $ 
  \end{itemize}

  \subsubsection{bernoulli}

  $X \sim Bernoulli(p) \quad \Rightarrow$ coin flip with probability $p$

  \begin{tightcenter}
    $E(X_i) = p \qquad \var(X_i) = p(1-p)$
    $E(S_n) = np \qquad \var(S_n) = np(1-p)$
  \end{tightcenter}

  \subsubsection{binomial}

  $X \sim Bin(n, p) \quad \Rightarrow$ 
  $X_i \mathop{\sim}\limits^{i.i.d.} Bernoulli(p)$

  \begin{tightcenter}
    $E(X) = np, \quad \var(X) = np(1-p)$

    \( {\displaystyle{ E(X) = \sum^n_{k=1} k \binom{n}{k} p^k (1-p)^{n-k} }} \)  

    $\cov(X,n-X) = -\var(X)$
  \end{tightcenter}

  \subsubsection{multinomial}

  $X \sim Multinomial(n, \mathbf{p})$ 

  \begin{itemize}
    \item for $k$ outcomes $\seq[k]{E}$, $\; Pr(E_i) = p_i$.
      For some $1 \leq i \leq k$, $E_i$ occurs $X_i$ times in $n$ runs.
  \end{itemize}

  $(\seq[k]{X})$ has the \textbf{multinomial distribution:}

  \begin{tightcenter}
    \( {\displaystyle{ Pr(X_1 = x_1, \dots, X_k = x_k) = \binom{n}{\seq[k]{x}} \Pi^k_{i=1} p_i^{x_i} }} \)   
  \end{tightcenter}

  \begin{itemize}
    \item where $\binom{n}{\seq[k]{x}} = \frac{n!}{x_1! x_2! \dots x_k!}$
      \begin{itemize}
        \item combinatorially, \# of arrangements of $\seq[k]{x}$
        \item $\sum^n_{i=1}x_i = n, \quad x_i \ge 0$
      \end{itemize}
  \end{itemize}

  \begin{tightcenter}
    $E(X) = \left[\begin{smallmatrix}np_1 \\ np_2 \\ \svdots \\ np_k\end{smallmatrix}\right]$,  
    $\quad \var(X_i) = np_i (1-p_i)$

    $\var(X) =$ \textit{covariance matrix} $M$ with $m_{ij} = \begin{cases}
      \var(X_i) & \text{if }i=j
      \\ \cov(X_i, X_j) & \text{if }i \neq j
    \end{cases} $
  \end{tightcenter}

  \begin{itemize}
    \item $\cov(X_i, X_j) < 0$
    \item $X_i \sim Bin(n, p_i)$
    \item $X_i + X_j \sim Bin(n, p_i + p_j)$
  \end{itemize}


  \section{02. PROBABILITY (2)}

  \subsection{Mean Square Error (MSE)}

  $$MSE = E\{ (Y-c)^2 \}$$

  \begin{itemize}
    \item predicting Y:
      \\* $MSE = \var(Y) + \{ E(Y) - c\}^2$
      \begin{itemize}
        \item $\min MSE=\var(Y)$ when $c = E(Y)$
      \end{itemize}
    \item $Y$ and $X$ are correlated: 
      \\* $MSE = \var[Y \vert x] + \{ E[Y\vert x] - c \}^2$
      \\* $MSE = E[(Y-c)^2 \vert x] = E[\{ Y - E(Y) \}^2 \vert x]$
      \begin{itemize}
        \item $\min MSE = \var(Y \vert x)$ when $c = E[Y \vert x]$
        \item if $c = E(Y)$ instead of $E(Y \vert x)$ $\Rightarrow$ the MSE increases by $( E(Y \vert x) - E(Y) )^2$
      \end{itemize}
  \end{itemize}

  \subsubsection{mean MSE}

  $$\frac{1}{n} \sum^n_{i=1}\var[Y \vert x_i] \approx E\{ \var[Y \vert X] \}$$

  \subsubsection{random conditional expectations}

  let $X, Y$ be r.v.s.
  \begin{itemize}
    \item $E[Y \vert X]$ is a r.v. which takes value $E[Y \vert x]$ with probability/density $f_X(x)$
    \item $\var[Y \vert X]$ is a r.v. which takes value $\var[Y \vert x]$ with probability/density $f_X(x)$ 
  \end{itemize}

  \begin{tightcenter}
    $E(E[X_2 \vert X_1]) = E(X_2)$

    $\var(E[X_2 \vert X_1]) + E(\var[X_2 \vert X_1]) = \var(X_2)$
  \end{tightcenter}

  \subsection{CDF (cumulative distribution function)}

  for r.v. $X$, let $F(x) = P(X \leq x)$

  \begin{itemize}
    \item domain: $\mathbb{R}$; $\;$ codomain: $[0,1]$
  \end{itemize}

  \begin{tightcenter}
    $F(x) = \int^\infty_{-\infty} f(x) \dx$
  \end{tightcenter}

  \subsection{Standard Normal Distribution}

  \begin{tightcenter}
    $Z \sim N(0, 1)$ has density function \\* 
    \( {\displaystyle{ \phi(z) = \frac{1}{\sqrt{2\pi}} \exp \{ -\frac{z^2}{2} \}, \quad -\infty < z < \infty }} \) 
  \end{tightcenter}

  $$E(Z) = 0, \quad \var(Z) = 1$$

  \begin{tightcenter}
    \textbf{CDF}, $\Phi(x) = P(Z \leq x) = \int^x_{-\infty} \phi(z) \mathop{dz}$
  \end{tightcenter}

  \begin{itemize}
    \item  $E(Z) = \int^\infty_{-\infty} z\phi(z) \mathop{dz} = 0$
      \begin{itemize}
        \item  $E(Z^2) = \int^\infty_{-\infty} z^2\phi(z) \mathop{dz} = 1$
          \item $E(Z^{2k+1}) = 0 \quad \forall k \in \mathbb{Z}_{\ge 0}$
      \end{itemize}
  \end{itemize}

  \subsubsection{general normal distribution}

  let  $X \sim N(\mu, \sigma^2)$ and $Y \sim N(\nu, \tau^2)$

  \begin{tightcenter}
    \textbf{standardisation:} $\frac{X - \mu}{\sigma} \sim N(0, 1)$
  \end{tightcenter}

  \begin{itemize}
    \item summations: 
      \begin{itemize}
        \item for constants $a, b \neq 0$, 
          \\* $\quad \; a + bX \sim N(a+b\mu, \; b^2\sigma^2)$
        \item $X + Y \sim N(\mu + \nu, \; \sigma^2 + \tau^2 + 2\cov(X,Y))$
          \begin{itemize}
            \item $\cov(X, Y) = 0$, $\quad \Rightarrow \quad$ $X \perp Y$
            \item $X \perp Y$ $\quad \Rightarrow \quad$ $X + Y \sim N(\mu + \nu, \; \sigma^2 + \tau^2)$
          \end{itemize}
      \end{itemize}

    \item for $W = a + bX$, 
      \begin{itemize}
        \item density,  $f_W(w) = \frac{d}{dw} F_W(w)$
        \item CDF, $\;\;\; F_W(w) = P(X \leq \frac{w-a}{b}) = \Phi(\frac{w-a}{b})$
      \end{itemize}
  \end{itemize}

  \subsection{Central Limit Theorem}

  let $\seq{X}$ be iid rv's with expectation $\mu$ and SD $\sigma$, with $ S_n = \sum^n_{i=1} X_i $

  \begin{tightcenter}
    \textbf{CLT} 
    \\* as $n \to \infty$, the distribution of the standardised $S_n = \frac{S_n - n\mu}{\sqrt{n}\sigma}$ converges to $N(0,1)$
  \end{tightcenter}

  \begin{itemize}
    \item $E(S_n) = n\mu$, $\; \var(S_n) = n\sigma^2$
    \item for large $n$, approximately $S_n \sim N(n\mu, n\sigma^2)$
  \end{itemize}

  \subsubsection{bernoulli}

  let $X_i \sim Bernoulli(p)$. then $S_n \sim Binom(n, p)$
  \begin{itemize}
    \item for large $n$, $S_n = N(np, np(1-p))$
    \item CLT: standardised $\frac{S_n - np}{\sqrt{n}\sqrt{p(1-p)}} \to N(0,1)$ as $n \to \infty$
  \end{itemize}

  \subsection{Distributions}

  \subsubsection{chi-square ($\chi^2$)}

  let $Z \sim N(0,1)$. 
  $\quad\Rightarrow$ then $Z^2 \sim \chi^2_1$

  \begin{itemize}
    \item $Z^2$ has $\chi^2$ distribution with 1 degree of freedom
    \item degrees of freedom = number of RVs in the sum
  \end{itemize}

  \begin{tightcenter}
    $E(Z^2) = 1, \quad E(Z^4) = 3$
    \\* $\var(Z^2) = E(Z^4) - \{ E(Z^2) \}^2 = 2$
  \end{tightcenter}

  let $\seq{V}$ be iid $\chi^2_1$ RVs and $V = \sum^n_{i=1}V_i$. then 
  \begin{tightcenter}
    $V \sim \chi^2_n$

    $E(V) = n \quad \var(V) = 2n$
  \end{tightcenter}

  \subsubsection{gamma}

  \begin{tightcenter}
    let $\alpha, \lambda > 0$. The $Gamma(\alpha, \lambda)$ density is 
    \\* \( {\displaystyle{ \frac{\lambda^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\lambda x}, \quad x > 0 }} \) 
    \\* where $\Gamma(\alpha)$ is a number that makes density integrate to 1
  \end{tightcenter}

  \begin{itemize}
    \item $\chi^2_n$ RV $\sim Gamma(\frac{n}{2}, \frac{1}{2})$
      \begin{itemize}
        \item $\chi^2_n$ is a special case of Gamma!
        \item density of $\chi_1^2$ RV $= \frac{1}{\sqrt{2\pi}} v^{-1/2} e^{-v/2}, \quad v > 0$
          \\* $\qquad\qquad\qquad\quad\;\; = Gamma(\frac{1}{2}, \frac{1}{2})$
      \end{itemize}
    \item if $X_1 \sim Gamma(\alpha_1, \lambda)$ and $X_2 \sim Gamma(\alpha_2, \lambda)$ are independent, then $X_1 + X_2 \sim Gamma(\alpha_1 + \alpha_2, \lambda)$
  \end{itemize}

  \subsubsection{t distribution}

  let $Z \sim N(0, 1)$ and $V \sim \chi^2_n$ be independent. 

  \begin{tightcenter}
    $\frac{Z}{\sqrt{V/n}} \sim t_n $
    \\* has a $t$ distribution with $n$ degrees of freedom.
  \end{tightcenter}

  \begin{itemize}
    \item $t$ distribution is symmetric around 0
    \item $t_n \to Z \;$ as $\; n \to \infty$ (because $\frac{V}{n} \to 1$)
  \end{itemize}

  \subsubsection{\textit{F} distribution}

  let $V \sim \chi^2_m$ and $W \sim \chi^2_n$ be independent. 

  \begin{tightcenter}
    $\frac{V/m}{W/n} \sim F_{m,n}$ 
    \\* has an $F$ distribution with $(m,n)$ degrees of freedom.
  \end{tightcenter}

  \begin{itemize}
    \item even if $m=n$, still two RVs $V,W$ as they are independent
    \item for $T \sim t_n$, $T^2 = \frac{Z^2}{V/n} \sim F_{1,n}$
  \end{itemize}

  \subsection{IID Random Variables}

  let $\seq{X}$ be iid RVs with mean $\bar{X}$. 

  \begin{tightcenter}
    \textbf{sample variance}, \( {\displaystyle{ S^2 = \frac{1}{n-1} \sum^n_{i=1} (X_i - \bar{X})^2 }} \) 

    $S$ is an estimate of $\sigma$
  \end{tightcenter}

  let $\seq{X}$ be iid $N(\mu, \sigma^2)$. $\bar{X} = \frac{1}{n} \sum^n_{i=1} X_i$. 
  \begin{tightcenter}
    $\bar{X} \sim N(\mu, \frac{\sigma^2}{n})$

    $E(\bar{X}) = \mu$, $\quad\var(\bar{X}) = \frac{\sigma^2}{n}$
  \end{tightcenter}

  more distributions:
  \begin{tightcenter}
    $\frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0,1)$

    $\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}$

    $\frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}$
  \end{tightcenter}

  \begin{itemize}
    \item $\bar{X}$ and $S^2$ are independent
  \end{itemize}

  \subsection{Multivariate Normal Distribution}

  let $\bm{\mu}$ be a $k \times 1$ vector and $\bm{\Sigma}$ be a \textit{positive-definite} symmetric $k \times k$ matrix.

  \begin{tightcenter}
    the random vector $\bm{X} = (\seq[k]{X})'$ has a multivariate normal distribution $N(\bm{\mu}, \bm{\Sigma})$ if its density function is 

    \( {\displaystyle{ \frac{1}{(2\pi)^{k/2} \sqrt{det\bm{\Sigma}}} \exp \left(-\frac{(\bm{x} - \bm{\mu})' \bm{\Sigma}^{-1}(\bm{x} - \bm{\mu})}{2}\right) }} \) 
  \end{tightcenter}

  \begin{itemize}
    \item $E(\bm{X}) = \bm{\mu}, \quad \var(\bm{X}) = \bm{\Sigma}$
    \item for any non-zero $k \times 1 $ vector $\bm{a}$, 
      \begin{tightcenter}
        $\bm{a'X} \sim N(\bm{a'\mu}, \bm{a'\Sigma a})$
      \end{tightcenter}
      \begin{itemize}
        \item $\bm{a}'\bm{\Sigma a} > 0$ because $\bm{\Sigma}$ is positive-definite
        \item the product $\bm{a'X}$ is a scalar (same for $\bm{a'\mu}, \bm{a'\Sigma a}$)
      \end{itemize}
    \item two multinomial normal random vectors $\bm{X}_1$ and $\bm{X}_2$, sizes $h$ and $k$, are independent if $\cov (\bm{X}_1, \bm{X}_2) = \bm{0}_{h \times k}$
      \begin{itemize}
        \item $(X_1-\Xbar, \dots, X_n - \Xbar)$ has a multivariate normal distribution; $\;$
          the covariance between $\Xbar$ and $(X_1-\Xbar, \dots, X_n - \Xbar)$ is $0$, thus they are independent
      \end{itemize}
  \end{itemize}

  \section{03. POINT ESTIMATION}

  for a variable $v$ in population $N$, 
  \begin{tightcenter}
    \( {\displaystyle{ 
        \mu = \frac{1}{N} \sum^N_{i=1} v_i
        \quad \quad 
        \sigma^2 = \frac{1}{N} \sum^N_{i=1} (v_i - \mu)^2
    }} \) 
  \end{tightcenter}

  \begin{itemize}
    \item $\mu, \;\sigma^2$ are \ildefinition{parameters} (unknown constants)
    \item a \textbf{simple random sample} is used to estimate parameters: individuals drawn from the population at random without replacement
  \end{itemize}

  \subsubsection{binary variable}

  for variable $v$ with proportion $p$ in the population,

  \begin{tightcenter}
    $\mu = p, \qquad \sigma^2 = p(1-p)$
  \end{tightcenter}

  \subsubsection{single random draw}

  for variable $v$ (population of size $N$, mean $\mu$, variance $\sigma^2$),
  let $X$ be the chosen $v$-value.

  \begin{tightcenter}
    $E(X) = \mu$, $\qquad \var(X) = \sigma^2$
  \end{tightcenter}

  \subsubsection{draws with replacement}

  let $\seq{X}$ be random draws with replacement from a population of mean $\mu$ and variance $\sigma^2$.

  \begin{tightcenter}
    random sample mean, \( {\displaystyle{ \Xbar = \frac{1}{n}\sum^n_{i=1}X_i }} \) 

    $\seq{X}$ are iid with $E(X_i) = \mu$, $\var(X_i) = \sigma^2$

    $E(\Xbar) = \mu$, $\var(\Xbar) = \frac{\sigma^2}{n}$
  \end{tightcenter}

  let $\seq{x}$ be realisations of $n$ random draws with replacement from the population.

  \begin{tightcenter}
    sample mean, \( {\displaystyle{ \xbar = \frac{1}{n} \sum^n_{i=1} x_i }} \) 

  \end{tightcenter}

  \begin{itemize}
    \item as $n \to \infty$, $\xbar \to \mu$  (LLN)
    \item sample distribution, $x_i$ has the same distribution as $X_i$ and the population distribution 
  \end{itemize}

  \subsubsection{representativeness}

  \begin{itemize}
    \item $\seq{X}$ is \textbf{representative} of the population
      \begin{itemize}
        \item as $n$ gets larger, $\Xbar$ gets closer to $\mu$
      \end{itemize}
    \item $\seq{x}$ are \textit{likely} representative of the population
  \end{itemize}

  \subsection{estimating mean}

  given data $\seq{x}$, 

  \begin{itemize}
    \item sample mean, $ \xbar = \frac{1}{n} \sum^n_{i=1} x_i $ is an  \ildefinition{estimate} of $\mu$
    \item the error in $\xbar$ is $\mu - \xbar$; $\quad$ it cannot be estimated
    \item $\xbar$ is a realisation of the \textbf{estimator} $\Xbar$
      \begin{itemize}
        \item this realisation is used to estimate $\mu$
      \end{itemize}
  \end{itemize}

  \subsubsection{standard error}

  the size of error in estimate $\xbar$ is roughly \( {\displaystyle{ SD(\Xbar) = \frac{\sigma}{\sqrt{n}} }} \)   

  the \textbf{standard error} (SE) in $\xbar$ is $\frac{\sigma}{\sqrt{n}}$

  \begin{itemize}
    \item SE is a constant by definition: $SE = SD(\hat{X}) = \frac{\sigma}{\sqrt{n}}$
  \end{itemize}

  \subsubsection{estimating $\sigma$}

  intuitive estimate of $\sigma^2$, $\hat{\sigma}^2 = \frac{1}{n} \sum^n_{i=1} (x_i-\xbar)^2 $

  \begin{tightcenter}
    sample variance, \( {\displaystyle{ s^2 = \frac{1}{n-1} \sum^n_{i=1} (x_i-\xbar)^2 }} \) 

    $E(s^2) = \sigma^2$
  \end{tightcenter}

  \section{Point estimation of mean}

  a population (size $N$) has unknown mean $\mu$, variance $\sigma^2$.

  for random draws (without replacement) $\seq{x}$: 

  $\xbar$ is a realisation of $\Xbar$, with $E(\Xbar) = \mu$, $\var(\Xbar) = \frac{\sigma^2}{n}$

  \begin{itemize}
    \item $\mu$ is estimated as $\xbar = \frac{1}{n} \sum^n_{i=1} x_i$ 
    \item error in $\xbar$ is measured by the SE: $\frac{\sigma}{\sqrt n} = SD(\Xbar)$
    \item SE is estimated as $\frac{s}{\sqrt n}$
  \end{itemize}

  $\Rightarrow$ $\mu$ is around $\xbar$, give or take $\frac{s}{\sqrt n}$ 

  \subsubsection{unbiased estimation}

  \begin{itemize}
    \item since $E(\Xbar) = \mu$, $\Xbar$ is an \textbf{unbiased} estimator of $\mu$. $\xbar$ is an unbiased estimate.
    \item $S^2$ is unbiased for $\sigma^2$: $E(S^2) = \sigma^2$
    \item $S$ is \textit{not} unbiased for $\sigma$: $E(S) < \sigma$
  \end{itemize}

  \subsection{Simple random sampling (SRS)}

  $n$ random draws \textit{without replacement} from a population of mean $\mu$ and variance $\sigma^2$. 

  \begin{itemize}
    \item for $i = 1, \dots, n$, $E(X_i) = \mu$ and $\var(X_i) = \sigma^2$ 
    \item for $i \neq j$, $\cov(X_i, X_j) = -\frac{\sigma^2}{N-1}$
  \end{itemize}

  \begin{itemize}
    \item if $n/N$ is relatively large, 
      \begin{itemize}
        \item multiply SE by correction factor $\sqrt \frac{N-n}{N-1}$
        \item standard error = $\frac{N-n}{N-1}$ 
      \end{itemize}
      $$E(\Xbar) = \mu, \quad \var(\Xbar) = \frac{N-n}{N-1}\frac{\sigma^2}{n}$$
    \item if $n << N$, then SRS is like sampling \textit{with replacement}
      (treat the data as if they come from IID RVs $\seq{X}$)
      $$E(\Xbar) = \mu, \quad \var(\Xbar) = \frac{\sigma^2}{n}$$
  \end{itemize}

  \subsubsection{estimating proportion $p$}

  \begin{itemize}
    \item in a 0-1 population, $\mu = p$, $\quad \sigma^2 = p(1-p)$
      \begin{itemize}
        \item $p$ is estimated as $\xbar$ (sample proportion of 1's)
      \end{itemize}
    \item $SE = \frac{\sqrt{p(1-p)}}{\sqrt n} = SD(\hat p)$
      \begin{itemize}
        \item estimated by replacing $p$ with $\xbar$
      \end{itemize}
    \item unbiased estimator $\hat p$
      \begin{itemize}
        \item $E(\hat p) = p, \quad \var(\hat p) = \frac{p(1-p)}{n}, \quad SD(\hat p) = SE$
      \end{itemize}
    \item the estimate of $\sigma$ is $\hat{\sigma}$, not $s$
    \item e.g. if a SRS of size 100 has 78 white balls, $p \approx 0.78 \pm \frac{\sqrt{0.78 \times 0.22}}{\sqrt {100}}$
  \end{itemize}

  \subsection{Gauss Model}

  Let $x_i$ be a realisation of $X_i$. $\seq[100]{X}$ are random draws with replacement from an imaginary population with mean $w$ and variance $\sigma^2$.
  $w$ and $\sigma^2$ are parameters (unknown constants).

  \begin{itemize}
    \item $E(X_i) = w, \quad \var{X_i} = \sigma^2$ (since $X_i$ is just 1 draw) 
    \item $E(\Xbar) = w, \quad \var{\Xbar} = \frac{\sigma^2}{100}$  
  \end{itemize}

  \section{04. ESTIMATION (SE, bias, MSE)}

  let $\seq{x}$ be from random draws $\seq{X}$ with replacement from a population of mean $\mu$ and variance $\sigma^2$.

  \begin{tightcenter}
    sample mean $\xbar$ is an \textit{unbiased estimate} of $\mu$

    $\xbar = \frac{1}{n} \sum^n_{i=1} x_i$

    SE = $\frac{\sigma}{\sqrt n} \approx \frac{s}{\sqrt n}$ tells us roughly how far $\xbar$ is from $\mu$

    sample variance, $s^2 = \frac{1}{n-1} \sum^n_{i=1} (x_i-\xbar)^2$
  \end{tightcenter}

  \subsection{MSE and bias}

  suppose measurements were from a population with mean $w + b$ where
  $b$ is a constant:
  $\quad x_i = w + b + \epsilon_i$
  \begin{itemize}
    \item $E(\Xbar) = w + b$
    \item $SD(\Xbar) = \frac{\sigma}{\sqrt n}$
      \begin{itemize}
        \item $SE = \frac{\sigma}{\sqrt n}$ measures how far $\xbar$ is from $w+b$, not $w$
      \end{itemize}
    \item if $b\neq 0$, then $\xbar$ is a biased estimate for $w$
  \end{itemize}

  \begin{tightcenter}
    $MSE = E\{(\Xbar - w)^2\} = \frac{\sigma^2}{n} + b^2$

    $MSE = SE^2 + bias^2$
  \end{tightcenter}

  as $n \to \infty, \; MSE \to b^2$

  \subsubsection{conclusion}

  \begin{tightcenter}
    let $\theta$ be a parameter (constant) and $\hat \theta$ be an estimator (RV).

    $SE = SD(\hat\theta)$, bias = $E(\hat\theta) - \theta$, 

    MSE = $E\{(\hat\theta - \theta)^2 = SE^2 + bias^2\}$
  \end{tightcenter}

  \section{05. INTERVAL ESTIMATION}

  let $\seq{x}$ be realisations of IID RVs $\seq{X}$ with unknown $\mu = E(X_i)$ and $\sigma^2 = \var(X_i)$. 

  sample mean, $\xbar = \frac{1}{n} \sum^n_{i=1}x_i$

  sample variance, $s^2 = \frac{1}{n-1} \sum^n_{i=1}(x_i-\xbar)^2$

  standard error, $SE = \frac{s}{\sqrt n}$

  \begin{tightcenter}
    \textbf{point estimation:} $\mu \approx \xbar$, give or take $\frac{s}{\sqrt n}$

    \textbf{interval estimation:} interval contains $\mu$ with some confidence level
  \end{tightcenter}

  interval estimation works well if
  \begin{itemize}
    \item $X_i$ has a normal distribution, for any $n > 1$ 
    \item $X_i$ has any other distribution but $n$ is large
  \end{itemize}

  \subsection{normal ''upper-tail quantile'' $z_p$}

  \begin{tightcenter}
    let $Z \sim N(0,1)$. for $0 < p < 1$, let $z_p$ be such that 

    $p=\Pr(Z > z_p)$
  \end{tightcenter}

  \begin{itemize}
    \item e.g. $z_{0.5} = 0$
    \item $z_p = (1-p)$-quantile of $Z$ 
    \item for $0<p<0.5$, $\Pr(-z_p \leq Z \leq z_p) = 1-2p$
  \end{itemize}

  \subsubsection{(case 1) normal distribution with known $\sigma^2$}

  assume $\seq{X}$ are IID $\sim N(0,1)$ with known $\sigma^2$.

  for $0<\alpha<1$, $\;\Pr(-z_{\frac{\alpha}{2}} \leq Z \leq z_{\frac{\alpha}{2}}) = 1-\alpha$

  \begin{tightcenter}
    \textbf{confidence interval for $\mu$:}
    the random interval 

    \( {\displaystyle{ \left( \Xbar-z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt n}, \Xbar+z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt n} \right) }} \) 

    contains $\mu$ with probability $1-\alpha$,

    and produces the realisation 
    $ \left( \xbar-z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt n}, \xbar+z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt n} \right) $
  \end{tightcenter}

  \begin{itemize}
    \item $1-\alpha$ is the \textbf{confidence level}
    \item \textit{Proof.} since $\frac{\Xbar - \mu}{\sigma / \sqrt n} \sim N(0,1)$,
      \begin{itemize}
        \item $\quad \Pr(-z_{\frac{\alpha}{2}} \leq \frac{\Xbar - \mu}{\sigma / \sqrt n} \leq z_{\frac{\alpha}{2}}) = 1 - \alpha$
          % \item $\quad \Pr(\mu-z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt n} \leq \Xbar \leq \mu + z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt n}) = 1 - \alpha$

        \item $\quad \Pr(\Xbar-z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt n} \leq \mu \leq \Xbar + z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt n}) = 1 - \alpha$
      \end{itemize}
  \end{itemize}

  \subsubsection{(case 2) normal distribution with unknown $\sigma^2$}

  assume $\seq{X}$ are IID $\sim N(\mu, \sigma^2)$ with unknown $\sigma^2$.

  replace $\sigma$ with $S$:

  \begin{tightcenter}
    for $0 < p < 1$, let $t_{p,n}$ be such that 

    $\Pr(t_n > t_{p,n}) = p$
  \end{tightcenter}

  \begin{itemize}
    \item $t_{p,n}$ is the \textit{upper} $p$ quartile of the $t$ distribution with $n$ degrees of freedom
      \begin{itemize}
        \item e.g. $t_{0.1,5} = 1.48$ (using $qt(0.9,5)$)
      \end{itemize}
    \item as $n \to \infty, \;\; t_{n,p} \to z_p$
    \item $\frac{\Xbar - \mu}{S/\sqrt n} \sim t_{n-1}$
    \item $\Pr( \Xbar - t_{\frac{\alpha}{2}, n-1} \frac{S}{\sqrt n} \leq \mu \leq \Xbar + t_{\frac{\alpha}{2}, n-1} \frac{S}{\sqrt n})$
  \end{itemize}

  \begin{tightcenter}
    the random interval

    $\left(\Xbar - t_{\frac{\alpha}{2}, n-1} \frac{S}{\sqrt n},  \Xbar + t_{\frac{\alpha}{2}, n-1} \frac{S}{\sqrt n}\right)$

    contains $\mu$ with probability $1-\alpha$.
  \end{tightcenter}

  \begin{itemize}
    \item data $\seq{x}$ give realisations $\xbar$ of $\Xbar$ and $s$ of $S$, thus the random interval gives a $(1-\alpha)$-CI for $\mu$: 
      $\left(\xbar - t_{\frac{\alpha}{2}, n-1} \frac{s}{\sqrt n},  \xbar + t_{\frac{\alpha}{2}, n-1} \frac{s}{\sqrt n}\right)$
  \end{itemize}

  \subsubsection{(case 3) general distribution with unknown $\sigma^2$}

  IID $\seq{X}$ with $E(X_i) =\mu$, $\var(X_i) = \sigma^2$ unknown

  \begin{itemize}
    \item for large $n$, approximately $\frac{S_n - n\mu}{\sqrt n \sigma} \sim N(0,1)$
    \item since $\frac{S_n - n\mu}{\sqrt n \sigma} \sim N(0,1) =  \frac{\Xbar - \mu}{\sigma / \sqrt n}$,
      \begin{itemize}
        \item $\quad \Pr(-z_{\frac{\alpha}{2}} \leq \frac{\Xbar - \mu}{\sigma / \sqrt n} \leq z_{\frac{\alpha}{2}}) \approx 1 - \alpha$
        \item $\quad \Pr(\Xbar-z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt n} \leq \mu \leq \Xbar + z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt n}) \approx 1 - \alpha$
      \end{itemize}
  \end{itemize}

  \begin{tightcenter}
    for large $n$, the random interval

    \( {\displaystyle{ \left( \Xbar-z_{\frac{\alpha}{2}}\frac{S}{\sqrt n}, \Xbar+z_{\frac{\alpha}{2}}\frac{S}{\sqrt n} \right) }} \) 

    contains $\mu$ with probability $\approx 1-\alpha$
  \end{tightcenter}

  \begin{itemize}
    \item data $\seq{x}$ give realisations $\xbar$ of $\Xbar$ and $s$ of $S$.
    \item $ \left( \xbar-z_{\frac{\alpha}{2}}SE, \xbar+z_{\frac{\alpha}{2}}SE \right) $ \\* is an \textit{approximate} $(1-\alpha)$-CI for $\mu$.
      \begin{itemize}
        \item $SE = \frac{s}{\sqrt n}$
        \item for SRS, multiply $SE$ by correction factor $\sqrt{\frac{N-n}{N-1}}$
      \end{itemize}
    \item contains $\mu$ with probability $<1-\alpha$
    \item probability $\to 1-\alpha$ as $n \to \infty$
    \item \textbf{exception}: for Bernoulli, $\sigma = \sqrt{p(1-p)}$ is not estimated by $s$, but by replacing $p$ with the sample proportion
  \end{itemize}

  \section{06. METHOD OF MOMENTS}

  modified notation of mass/density functions:

  \begin{itemize}
    \item \textbf{bernoulli}: $f(x\vert p) = p^x (1-p)^{1-x}, \quad x=0,1$
      \begin{itemize}
        \item parameter space is $(0,1)$
      \end{itemize}
    \item \textbf{poisson}: $f(x \vert \lambda) = \frac{\lambda^xe^{-\lambda}}{x!}, \quad x=0,1,\dots$
      \begin{itemize}
        \item parameter space is $\mathbb{R}_+$
      \end{itemize}
  \end{itemize}

  \subsection{parameter estimation}

  assuming data $\seq{x}$ are realisations of IID RVs $\seq{X}$ with mass/density function  $f(x \vert \theta)$, 
  where $\theta$ is unknown in parameter space $\Theta$.

  \begin{itemize}
    \item 2 methods to estimate $\theta$ :
      \begin{itemize}
        \item method of moments (MOM)
        \item method of maximum likelihood (MLE)
      \end{itemize}
    \item for both:
      \begin{itemize}
        \item the estimate of $\theta$ is a realisation of an estimator $\hat \theta$ 
        \item SE is $SD(\hat \theta)$
        \item bias is $E(\hat \theta) - \theta$
      \end{itemize}
    \item parameter space $\Theta$: set of values that can be used to estimate the real parameter value $\theta$
  \end{itemize}

  \subsection{Moments of an RV}

  \begin{tightcenter}
    the $k$-th moment of an RV $X$ is

    $\mu_k = E(X^k)$, $\quad k = 1,2,\dots$
  \end{tightcenter}

  \subsubsection{estimating moments}

  let $\seq{X}$ be IID with the same distribution as $X$. 

  \begin{tightcenter}
    the $k$-th sample moment  is

    $\hat{\mu}_k = \frac{1}{n} \sum^n_{i=1}X_i^k$
  \end{tightcenter}

  \begin{itemize}
    \item $E(\hat\mu_k) = \mu_k$  $\quad\Rightarrow$ unbiased estimator!
    \item $\hat{\mu}_k$ is an estimator of $\mu_k$. For realisations $\seq{x}$, the realisation $\frac{1}{n}\sum^n_{i=1} x^k_i$ is an \textit{unbiased} estimate of $\mu_k$.
    \item hat ($\hat{\phantom x}$) means estimator (random variable)
      \begin{itemize}
        \item note that this violates the uppercase=RV, lowercase=(fixed)realisation notation
        \item $\xbar = \frac{1}{n} \sum^n_{i=1} x_i$
      \end{itemize}
  \end{itemize}

  \subsubsection{MOM: Poisson}

  assume $\seq{x}$ are realisations of IID $Poisson(\lambda)$ RVs $\seq{X}$.
  Let $\lambda$ be the mean number of emissions per 10 seconds ($\lambda$ is a parameter).

  \begin{itemize}
    \item let $X \sim Poisson(\lambda)$. $\mu_1 = \lambda$. Estimate $\lambda$ by estimating $\mu_1$ using sample mean $\xbar$, which is an estimator of  $\Xbar$.
    \item the MOM estimator is $\hat \lambda = \hat{\mu_1} = \Xbar$
      \begin{itemize}
        \item the random sample mean
      \end{itemize}
    \item $\var(X) = \lambda$, $\var(\Xbar) = \frac{\lambda}{n}$, SE = SD of estimator $= \sqrt \frac{\lambda}{n}$
  \end{itemize}

  \begin{tightcenter}
    $\lambda \approx \xbar \pm \sqrt\frac{\lambda}{n}$
  \end{tightcenter}

  \subsubsection{MOM: Bernoulli}

  Assume $\seq{X}$ are iid $Bernoulli(p)$ RVs. 

  Finding MOM estimator of $p$:

  \begin{itemize}
    \item let $X \sim Bernoulli(p)$. $\quad\Rightarrow \mu_1 = p$
    \item MOM estimator, $\hat p = \hat{\mu_1} = \Xbar$
      \begin{itemize}
        \item random sample proportion of 1's
      \end{itemize}
    \item SE = SD of estimator $=\sqrt{\var(\hat p)} = \sqrt \frac{p(1-p)}{n}$
  \end{itemize}

  \subsubsection{MOM: Normal}

  let $\seq{X}$ be iid $N(\mu, \sigma^2)$ with parameters $\mu$, $\sigma^2$

  for $X\sim N(\mu, \sigma^2)$: $\quad$ parameter space, $\Theta = \mathbb{R} \times \mathbb{R}_+$

  \begin{enumerate}
    \item $\mu_1 = \mu, \quad \mu_2 = \sigma^2 + \mu^2$
    \item express $\mu = \mu_1$; $\; \sigma^2 = \mu_2 - \mu_1^2$; then add hats
    \item MOM estimators: 
      \begin{tightcenter}
        $\hat\mu = \Xbar$
        $\qquad$
        $\hat{\sigma^2} = \frac{1}{n} \sum^n_{i=1} (X_i - \Xbar)^2$
      \end{tightcenter}
  \end{enumerate}

  (to construct CI for $\sigma^2$: use  $S^2$ $\quad \Rightarrow$ since $E(S^2) = \sigma^2$)

  \subsubsection{MOM: Geometric}

  let $\seq{x}$ be realisations of IID  $Geometric(p)$ RVs $\seq{X}$ with expectation $1/p$.

  \begin{itemize}
    \item for $X \sim Geometric(p) \quad \Rightarrow E(X) = \frac{1}{p}$
      \begin{itemize}
        \item $\Pr (X=i) = p(1-p)^{i-1}$ for $i=1,2,\dots$
        \item $E(X) = \sum^\infty_{i=1} ip(1-p)^{i-1} = \frac{1}{p}$
      \end{itemize}
    \item $\mu_1 = \frac{1}{p} \quad \Rightarrow p = \frac{1}{\mu_1} \quad \Rightarrow \hat p = \frac{1}{\Xbar}$
    \item MOM estimator, $\hat p = \frac{1}{\Xbar}$
      \begin{itemize}
        \item then MOM estimate $= \frac{1}{\xbar}$
      \end{itemize}
    \item SE = $SD(1/\Xbar) \qquad \Rightarrow$ use monte carlo to approximate
  \end{itemize}

  \subsubsection{MOM: Gamma}

  let $\seq{X}$ be iid $Gamma(\alpha, \lambda)$ RVs with shape parameter $\alpha>0$, rate parameter $\lambda>0$

  \begin{itemize}
    \item $X \sim Gamma(\alpha, \lambda)$, $\; E(X) = \frac{\alpha}{\lambda}, \; E(X^2) = \frac{\alpha(\alpha+1)}{\lambda^2}$ 
    \item express parameters in terms of moments: $\mu_1 = \frac{\alpha}{\lambda},\; \mu_2 - \mu_1^2 = \frac{\alpha}{\lambda^2}$
      $\quad\Rightarrow \lambda = \frac{\mu_1}{\mu_2 - \mu_1^2}, \alpha = \lambda\mu_1$ 
    \item MOM estimators: $\hat\alpha = \frac{\Xbar^2}{\hat\sigma^2}, \; \hat \lambda = \frac{\Xbar}{\hat \sigma^2}$
  \end{itemize}

  \subsubsection{MOM estimators are consistent}

  let $\seq{X}$ be iid with mass/density $f(x \vert \theta)$, where $\theta \in \Theta \subset \mathbb{R}$.

  Suppose $\theta = g(\mu_1)$ for some \textit{continuous} function $g$. 

  Then the MOM estimator is \textbf{consistent} (approaches $\theta$ with more data) 

  \begin{itemize}
    \item the MOM estimator is $\hat \theta = g(\hat\mu_1)$. as $n \to \infty, \; \hat\mu_1 \to \mu_1$
    \item since $g$ is continuous, $\quad \hat\theta \to g(\mu_1) = \theta$ 
      \begin{itemize}
        \item \ildefinition{asymptotic unbiasedness}: $E(\hat\theta) \to \theta$
      \end{itemize}
  \end{itemize}

  \section{07. MLE}

  MOM: works through estimating moments - if no formula is available for $SD(\hat \theta)$ or $E(\hat\theta)$, monte carlo can be used

  MLE: another estimation method

  \subsection{Likelihood function}

  let $\seq x$ be realisations of iid rvs $\seq X$ with density $f(x \vert \theta), \; \theta \in \Theta \subset \mathbb{R}^k$.

  \begin{tightcenter}
    \ildefinition{likelihood function} $L : \Theta \to \mathbb{R}_+$ is
      $L(\theta) = f(x_1 \vert \theta) \times \dots \times f(x_n \vert \theta) 
      = \prod^n_{i=1} f(x_i \vert \theta)$

    \ildefinition{loglikelihood function} $\ell : \Theta \to \mathbb{R}$ is
      $\ell(\theta) = \log L(\theta) = \sum^n_{i=1}\log f(x_i\vert \theta)$
  \end{tightcenter}

  \subsection{Maximum Likelihood Estimation (MLE)}

  $$L(\theta) = \prod^n_{i=1} f(x_i \vert \theta)$$ 

  \begin{itemize}
    \item \definition[of $L$]{maximiser} the maximum likelihood estimate of $\theta$ (a realisation of the MLEstimator $\hat\theta$)
  \end{itemize}

  \subsubsection{poisson (log)likelihood/MLE}

  $Poisson(\lambda): f(x \vert \lambda) = \frac{\lambda^2e^{-\lambda}}{x!}, \; x = 0,1,2,\dots$

  \begin{itemize}
    \item let $\seq{x}$ be realisations of iid Poisson($\lambda$) RVs $\seq X$. 
      the joint probability of data is  $f(x_1 \vert \lambda) \times \dots \times f(x_n\vert \lambda) = \frac{\lambda^{\sum^n_{i=1} x_i} e^{-n\lambda}}{x_1! \dots x_n!}$
    \item \textbf{likelihood}: probability as a function of only $\lambda$ 
      $L(\lambda) = \frac{\lambda^{\sum^n_{i=1} x_i} e^{-n\lambda}}{x_1! \dots x_n!}$
    \item \textbf{loglikelihood:} $\ell(\lambda) = (\sum^n_{i=1} x_i) \log \lambda - n\lambda - \sum^{n}_{i=1}\log(x_i!)$
    \item \textbf{MLE of $\lambda$} $= \xbar$ $\quad$ (maximiser of $L(\lambda)$)

      \begin{itemize}
        \item differentiate $\ell(\lambda)$: $\ell'(\lambda) = \frac{\sum^{n}_{i=1} x_i}{\lambda} - n$
        \item $\ell'(\lambda) = 0 \quad \Rightarrow \lambda = \xbar$ 
        \item $\ell''(\lambda) < 0$ (thus max point)
      \end{itemize}
  \end{itemize}

  \subsubsection{normal (log)likelihood/MLE}

  $N(\mu, \sigma^2)$: for $x \in \mathbb{R}$,

  $f(x \vert \mu, \sigma) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} = (2\pi)^{\frac{1}{2}}\sigma^{-1} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$

  \begin{itemize}
    \item let $\seq{x}$ be realisations of iid N($\mu, \sigma$) RVs $\seq X$. 
      the joint probability of data is  $f(x_1 \vert \lambda) \times \dots \times f(x_n\vert \lambda) = \frac{\lambda^{\sum^n_{i=1} x_i} e^{-n\lambda}}{x_1! \dots x_n!}$
    \item \textbf{likelihood} function: joint density as a function of $(\mu, \sigma)$
  \end{itemize}
  \begin{align*}
    L(\mu, \sigma) &= f(x_1 \vert \mu,\sigma) \times \dots \times f(x_n \vert \mu, \sigma) \\
                   &= (2\pi)^{-\frac{n}{2}} \sigma^{-n} e^{-\frac{1}{2\sigma^2}\sum^n_{i=1}(x_i - \mu)^2}
  \end{align*}

  \begin{itemize}
    \item \textbf{loglikelihood:}
      $\ell(\mu,\sigma) = -\frac{n}{2} \log(2\pi) - n\log\sigma - \frac{1}{2\sigma^2} \sum^n_{i=1}(x_i - \mu)^2$
    \item \textbf{MLE:}
      \begin{itemize}
        \item MLE of $\mu$ $= \xbar$
        \item MLE of $\sigma$ $= \hat \sigma = \sqrt{ \frac{1}{n} \sum^n_{i=1} (x_i - \xbar)^2 }$
      \end{itemize}
  \end{itemize}








\end{multicols*}

\end{document}
