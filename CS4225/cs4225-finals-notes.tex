\documentclass[10pt, landscape]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{calc}
\usepackage{multicol}
\usepackage[a4paper,margin=3mm,landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\usepackage{newtxtext} 
\usepackage{enumitem}
\usepackage[table]{xcolor}
\usepackage{mathtools}
\setlist{nosep}
% for including images
\graphicspath{ {./images/} }

\pdfinfo{
  /Title (CS4225.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Gavin Chiam)
  /Subject (CS4225)
/Keywords (CS4225, nus,cheatsheet, pdf, notes)}

% Turn off header and footer
\pagestyle{empty}

% redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
  {-1ex plus -.5ex minus -.2ex}%
  {0.5ex plus .2ex}%x
{\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
  {-1explus -.5ex minus -.2ex}%
  {0.5ex plus .2ex}%
{\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
  {-1ex plus -.5ex minus -.2ex}%
  {1ex plus .2ex}%
{\normalfont\small\bfseries}}%
\makeatother

\renewcommand{\familydefault}{\sfdefault}
\renewcommand\rmdefault{\sfdefault}
%  makes nested numbering (e.g. 1.1.1, 1.1.2, etc)
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\renewcommand\labelitemii{•}
\renewcommand\labelitemiii{•}

\definecolor{mathblue}{cmyk}{1,.72,0,.38}
\everymath\expandafter{\the\everymath \color{mathblue}}

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
%% adjust spacing for all itemize/enumerate
\setlength{\leftmargini}{0.5cm}
\setlength{\leftmarginii}{0.5cm}
\setlist[itemize,1]{leftmargin=2mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,2]{leftmargin=4mm,labelindent=1mm,labelsep=1mm}
\setlist[itemize,3]{leftmargin=4mm,labelindent=1mm,labelsep=1mm}

% adding my commands
\input{../commands/style-helpers.tex}

% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols*}{4}
  % multicol parameters
  \setlength{\columnseprule}{0.25pt}

  \begin{center}
    \fbox{%
      \parbox{0.8\linewidth}{\centering \textcolor{black}{
          {\Large\textbf{CS4225}}
        \\ \normalsize{AY24/25 SEM 1}}
        \\ {\footnotesize \textcolor{gray}{github/Gavino3o}}
      }%
    }
  \end{center}


  \section{Introduction}

  \begin{itemize}
    \item What is data science? The study of data. It involves developing methods of recording, storing, and analysing data to effectively extract useful information.
    \item Challenges of Big Data: Volume, Velocity, Variety, Veracity, Value.
    \item Cloud Computing: The delivery of computing services over the internet. Eg. storage, databases, networking, software, analytics, intelligence, etc.
    \item Virtualisation and Containers: \includegraphics[width=0.95\linewidth]{virtualisation_container.png}
    \item Infrustructure as a Service (IaaS): Provides virtualised computing resources over the internet. Eg. AWS, Azure, Google Cloud.
    \item Platform as a Service (PaaS): Provides a platform allowing customers to develop, run, and manage applications without the complexity of building and maintaining the infrastructure.
    \item Software as a Service (SaaS): Software that is centrally hosted and licensed on a subscription basis.
  \end{itemize}

  \subsubsection*{Data Centers}
  \begin{itemize}
    \item A facility composed of networked computers and storage that businesses or other organisations use to organise, process, store, and disseminate large amounts of data.
    \item Single Server: A single physical server that runs a single operating system.
    \item Rack: Contains multiple servers, oftern connected to a rack switch (or top-of-rack switch).
    \item Data Center: Contains multiple racks, often connected to a data center switch (or a core switch).
    \item Bandwidth: The maximum amount of data that can be transmitted per unit time (e.g 1 Gbps). [Large Files]
    \item Latency: The time it takes for data to travel from the source to the destination (one-way) or from source to destination and back to source (round trip). Measured in ms.  [Small Files]
    \item Throughput: Rate at which data is \textit{actually} transmitted accross the network during a period of time.
    \item Latency combines additatively. The lower the latency, the better the performance.
    \item Bandwidth is approximately the minimum bandwidth throughout the network path. The higher the bandwidth, the better the performance.
    \item Storage Hierarchy (Higher the storage capacity, the higher the network latency, lower the bandwidth as it is bottlenecked by switches)
    \item Data Flow Paths (Local Disk -> Local DRAM -> TOR Switch -> Core Switch ... etc)
    \item Disk read/write is slower than DRAM read/write. DRAM read/write is slower than L1/L2 Cache read/write.
    \item Latency increases over the storage hiearchy as we go from Local Server to Rack to Data Center.
    \item \textbf{Latency is dominated by disk speed. Disk transfer is very expensive, any transfer involving disks take about the same amount of time.}
    \item Bandwidth decreases over the storage hiearchy as we go from Local to Rack to Dat Center
    \item \textbf{Bandwidth tends to be bottlenecked by networking switches.}
    \item Worst Performance: Disk read/write for Latency. Network switch bottleneck for Bandwidth.
    \item Ideas for Massive Data Processing:
    \begin{itemize}
      \item Horizontal Scaling: Distributing the load accross multiple machines. Eg. Hadoop, Spark. Instead of vertical scaling (upgrading a single machine).
      \item Move processing to the data
      \item Process data sequentially and avoid random access as it involves disk seeks which leads to high latency.
      \item Seamless scalability: The ability to increase the capacity of the system without affecting the performance of the system.
    \end{itemize}
  \end{itemize}

  \section{MapReduce (Hadoop)}

  \begin{itemize}
    \item MapReduce is a programming model for processing and generating large data sets with a parallel, distributed algorithm on a cluster.
    \item Divide, Conquer, Aggregate.
    \item Challenges: Allocating Tasks, Scheduling, Partial Results, Aggregation, Synchronisation, Fault Tolerance.
    \item Synchronisation Barrier between Map phase and Reduce phase.
    \item Map Task is a basic unit of work. It is typically 128MB of data. At the beginning the input is broken into splits of 128MB. A map task is a job requiring to process one split, not a worker.
    \item Map Function is a user-defined function that processes a key/value pair to generate a set of intermediate key/value pairs.
    \item A map task can invoke the map function multiple times.
    \item Reduce Task is a basic unit of work. It is typically 1GB of data. A reduce task is a job requiring to process one partition, not a worker.
    \item Reduce Function is a user-defined function that processes a key and a set of values to generate a set of output values.
    \item A reduce task can invoke the reduce function multiple times.
    \item Data is \textbf{sorted by key and not value.}
    \item All values with the same key are sent to the same reduce task.
    \item Formally:
    \begin{itemize}
      \item \textbf{Map}: $(k_1, v_1) \rightarrow list(k_2, v_2)$
      \item \textbf{Reduce}: $(k_2, list(v_2)) \rightarrow list(k_3, v_3)$
    \end{itemize}
    \item MapReduce execution framework handles scheduling, shuffling, synchronisaton, faults. Everything happens on top of a distributed file system, HDFS.
    \item MapReduce execution steps:\newline Submit Job $\rightarrow$ Schedule Tasks $\rightarrow$ Read $\rightarrow$ MAP $\rightarrow$ Shuffle [Local Write, Remote Read] $\rightarrow$ REDUCE $\rightarrow$ Write.
    \item \textbf{Shuffle}: The process of transferring data from the map tasks to the reduce tasks. It involves sorting, partitioning, and copying intermediate data.
    \item \textbf{Partitioning (Local Write)}: The process of determining which reduce task will receive the intermediate data for a given key. Partitions are based of keys, a single partition can have multiple keys.
    \item \textbf{Sorting}: The process of sorting the intermediate data by key. Key value pairs arrive to the reduce task in order sorted by key.
    \item \textbf{Copying (Remote Read)}: The process of transferring the intermediate data from the map tasks to the reduce tasks. Reduce worker reads data from the \textbf{same corresponding partition from each Map worker}.
    \item Reduce phase is often the slowest phase as it involves network I/O and synchronisaton.
    \item The \textbf{Master Node} is responsible for scheduling tasks, monitoring tasks, and re-executing failed tasks. It does not execute tasks.
    \item The \textbf{Worker Node} is responsible for executing tasks. It does not schedule tasks. A single worker can handle multiple map tasks.
    \item \includegraphics[width=0.95\linewidth]{mapreduce_implementation.png}
    \item The number of:
    \begin{itemize}
      \item Map Function Calls = Number of Input Key-Value pairs
      \item Reduce Function Calls = Number of Unique Intermediate Keys
      \item Map Tasks = Number of Input Splits
      \item Reduce Tasks = Number of Partitions (Specified by user)
    \end{itemize}
  \end{itemize}

  \subsubsection*{Partitioner and Combiner}
  \begin{itemize}
    \item Task Straggler Problem: Some tasks take longer to complete than others. This can slow down the entire job.
    \item It can be caused by certain keys in the data set that have too high of a frequency compared to other keys.
    \item User can define a custom partitioner to partition the data in a way that reduces the load on the reduce tasks.
    \item Combiner is a mini-reduce function that runs on the map side. It is used to reduce the amount of data that needs to be transferred to the reduce tasks.
    \item Generally, ensure that the computation is associative and commutative to ensure that the correctness of the combiner.
    \item Combiners are invoked after the map function, during the local write phase (but before the actual disk write to save disk I/O).
    \item Paritioners are invoked during the local write phase, as this stage requires knowing which keys need to go to which reducer.
    \item Both paritioners and combiners are optional and can be defined by the user.
    \item Guideline for basic algorithmic design: Linear scalibility, minimise disk and network I/O, reduce memory working set of each task/worker.
  \end{itemize}

  \subsubsection*{Secondary Sort}

  \begin{itemize}
    \item In the Reduce phase, the intermediate data is sorted by key. However, we may want to sort the values for each key.
    \item Secondary Sort is the process of sorting the values for each key in the Reduce phase.
    \item Composite Key $(k_1, k_2, v)$ + Custom Comparator (Compare $k_1$; If equal, compare $k_2$) + Custom Partitioner (Partition by $k_1$ only)
    \item It is not suitable to used hash based partitioning as it will not guarantee that all values for a key will go to the same reducer.
    \item Since we defined the custom comparator to sort based on the composite key, the values for each key will be sorted.
    \item This ultises the fact that the intermediate data is sorted by key in the shuffle phase.
    \item Sorting data in the reducer is expensive as it involves disk I/O. Sorting data in the map phase is cheaper as it involves memory I/O.
  \end{itemize}

  \subsubsection*{Preserving State in MapReduce}
  \begin{itemize}
    \item In-mapper combiner: A combiner that runs within the map task. It is used to reduce the amount of data that needs to be transferred to the reduce tasks.
    \item Different calls to the function used in the mapping or reducing phase can preserve a state (e.g store variables) to be shared between the function calls.
    \item A key thing to note is that one of our goals is to reduce the amount of disk and memory IOs to maximise efficiency. However, with the context of the program, we will emit many intermediate results which incurs an IO each time.
    \item Word Count Example:
    \begin{itemize}
      \item Version 1: Using a hash table in the mapping function. In a way, the mapping is already doing combining job using the properties of a hash table. [IN-MAPPER COMBINER as opposed to regular combiner (externally defined)]
      \item For version 1, regular combiners are still used. As regular combiners can combine outputs for the whole map task, not just the map function. (I.e it can combine each hash table produced by each map function call)
      \item Version 2: A single hash table is stored for the execution of the map task in the Mapper object. Each map function call will update the shared hash table in the Mapper object.
      \item For version 2, a drawback is that the hash table is stored in memory (increased memory working set). If the hash table is too large, it may cause memory issues.
    \end{itemize}
  \end{itemize}

  \subsection{Hadoop Distributed File System (HDFS)}

  \begin{itemize}
    \item Conceptually similar to a regular file system, but with the data spread accross a cluster of machines.
    \item Same data is replicated across multiple machines to ensure fault tolerance. Default replication factor is 3.
    \item Single master node (NameNode) and multiple worker nodes (DataNodes). Centralised metadata management.
    \item File stored as blocks (default 128MB). Blocks are replicated across multiple DataNodes.
    \item Write Once, Read Many (WORM) model. Once a file is written, it cannot be modified. But files can be appended to.
    \item HDFS is optimised for large files. It is not suitable for small files as it incurs overhead for each file.
    \item HDFS is designed for high throughput, not low latency. It is not suitable for real-time or random access.
    \item \includegraphics[width=0.95\linewidth]{hdfs_reading_data.png}
    \item \includegraphics[width=0.95\linewidth]{hdfs_writing_data.png}
    \item NameNode: Manages the fiile system namespace (file/directory structure, metadata, file-to-block mapping, access permissions). Directs clients to datanodes. It does not store the data itself.
    \item NameNode also maintains overall health of the cluster through heartbeats, block rebalancing.
    \item Data Locality: \includegraphics[width=0.95\linewidth]{hadoop_data_locality.png}
  \end{itemize}

  \section{Relational Databases}

  \begin{itemize}
    \item Projection (SELECT): Map-only job. Output of the map function is the final output desired.
    \item Selection (WHERE): Map-only job. Predicate is applied in the map function.
    \item Aggregation (GROUP BY, COUNT, SUM, AVG): Map-Reduce job. Map function emits intermediate key-value pairs. Reduce function aggregates the values for each key.
    \item Join (INNER JOIN, OUTER JOIN):
      \begin{itemize}
        \item \textbf{Method 1: Braodcast Join (Map Join):}
        \item Small table is broadcasted to all worker nodes. Large table is partitioned and sent to worker nodes.
        \item The small table can be converted to a hash table and stored in memory.
        \item Mapper iterates through the large table and looks up the corresponding value in the hash table for the join.
        \item \textbf{Method 2: Common Join (Reduce-Side Join):}
        \item Slower than broadcast join as it involves network I/O, but it uses less memory.
        \item Different mapper tasks are used for each table. The reducer task is used to join the tables.
        \item The key of intermediate results is appended with \texttt{table\_id} to create a composite key. Secondary sort is used to ensure that keys from one table all arrive before keys from the other table.
        \item The reducer task will hold the intermediate results from a table in memory, then iterate through the values from the other table and perform the join.
      \end{itemize}
  \end{itemize}

  \section{Similarity Search}

  \begin{itemize}
    \item Near Neighbours: Objects that are similar to each other.
    \item Distance Measure: A function that measures the distance between two objects.
    \item Similarity Measure: A function that measures the similarity between two objects. Opposite of distance measure.
    \item Euclidean Distance: $\sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$
    \item Manhattan Distance: $\sum_{i=1}^{n} |x_i - y_i|$
    \item Cosine Similarity: $\frac{A \cdot B}{||A|| \cdot ||B||}$ where $A \cdot B = \sum_{i=1}^{n} A_i \cdot B_i$
    \item Cosine Similarity is often used for text data as it is invariant to the magnitude of the vectors. Only considers the direction of the vectors.
    \item Jaccard Similarity: $\frac{|A \cap B|}{|A \cup B|}$. Intersection over Union.
    \item Jaccard Distance = 1 - Jaccard Similarity
    \item \textbf{All pairs similarity search}: Given a large number of documents, find pairs of documents that are similar to each other.
    \item \textbf{Similarity search:} Given a query document, find the similar documents in the database.
    \item \textbf{Shingling:} A technique to convert a document into a set of shingles. A shingle is a sequence of words in a document. Imagine sliding window of size k over the document.
    \item \textbf{Min-Hashing:} A technique to estimate the Jaccard similarity between two sets. It is used to reduce the size of the shingle set.
  \end{itemize}

  \subsubsection{Shingles}
  \begin{itemize}
    \item Matrix Representation: Each row represents a shingle, each column represents a document. 
    \item A cell is 1 if the shingle is in the document, 0 otherwise.
    \item Intersection: Number of rows with both columns as 1. 
    \item Union: Number of rows with at least one column as 1.
    \item K-Shingle Set: Set of all possible k-shingles in a document.
  \end{itemize}

  \subsubsection{Min-Hashing}
  \begin{itemize}
    \item \textbf{Key Property:} The probability that two documents have the same min-hash value is equal to the Jaccard similarity of the documents.
    \item Formally: $P(h(A) = h(B)) = J(A, B)$
    \item \textbf{Key Idea:} Use a hash function to map the shingle set to a smaller set of hash values. The minimum hash value is used as the signature of the document.
    \item Multiple hash functions are used to generate multiple signatures for each document. The signatures are used to estimate the Jaccard similarity.
    \item Documents with Jaccard similarity above a threshold will be accepted as candidate pairs.
    \item Candidate pairs can themselves be the output or can be further verified using the original shingle set.
    \item \textbf{MapReduce Framework:}
    \begin{itemize}
      \item Map: Read over the document and extract the shingles. Hash each shingle and take the min of them to get the MinHash signature
      \item Emit \texttt{<signature, document\_id>}. \textit{Notice that the signature is the key}
      \item Redicue: Receive all documents with a given MinHash signature. Generate all candidate pairs from these documents.
      \item (Optional): Verify the candidate pairs using the original shingle set.
    \end{itemize}
  \end{itemize}

  \section{K-Means Clustering}

  \begin{itemize}
    \item K-Means is an iterative algorithm that partitions the data into K clusters.
         \begin{enumerate}
          \item Randomly initialise K cluster centroids.
          \item Assign each data point to the nearest cluster centroid.
          \item Recompute the cluster centroids based on the data points assigned to the cluster.
          \item Stop when the cluster centroids do not change significantly or no data points change clusters.
         \end{enumerate}
     \item Map Phase: Assign each data point to the nearest cluster centroid. Emit \texttt{<cluster\_id, point>}.
     \item Reduce Phase: Recompute the cluster centroids based on the data points assigned to the cluster. Emit \texttt{<cluster\_id, new\_centroid>}.
     \item Extending Point: Adding a dimension to the coordinate emitted which is just a 1 to help with the computation of the sum and count.
     \item V1: Emits each point with the cluster it belongs to without combining.
     \item Disk I/O exchanged: $O(nmd)$. Where n is the number of data points, m is the number of iterations, d is the number of dimensions.
     \item V2: In-mapper combiner combines points in the same cluster within the map task using a hash table. Emits each cluster and the list of points assigned to it in the current iteration.
     \item Disk I/O exchanged: $O(kmd)$. Where k is the number of clusters.
     \item \includegraphics[width=0.95\linewidth]{kmeans_mapreduce.png}
  \end{itemize}

  \section{NoSQL Databases}

  \begin{itemize}
    \item NoSQL databases are non-relational databases that provide a mechanism for storage and retrieval of data that is modelled in means other than the tabular relations used in relational databases.
    \item Stands for "Not Only SQL"
    \item Examples: Key-Value Stores, Document Stores, Wide Column Stores, Graph Databases, Vector Databases.
    \item Features: Horizontally Scalable, Replicated/Distributed over many servers, Schema-less, Eventual Consistency, High Availability, Simple call interface, often weaker concurrency model than RDBMS, Effcient use of distributed indexes and RAM, Flexible Schemas.
    \item Pros: Flexible/Dynamic Schema, Horizontal Scalability, High Performance and Availability
    \item Cons: No declarative query language, Weaker consistency guarantees.
  \end{itemize}

  \subsection{Key-Value Stores}
  \begin{itemize}
    \item \textbf{Keys} are usually primitive data types (ints, string, raw bytes, etc) and can be queried.
    \item \textbf{Values} can be any data type (string, JSON, XML, BLOB, etc) and are opaque to the database. It usually does not support querying.
    \item \textbf{Operations}: Get, Put,Multi-Get, Multi-Put, Range Queries.
    \item Suitable for:
    \begin{itemize}
      \item Small continuous reads and writes
      \item Storing basic information (user profiles, session information, caches, raw chunks of bytes). Or no clear schema.
      \item When compex queries are not needed.
      \item Non-persistent: Big in-memory hash table. Example: Memcached, Redis.
      \item Persistent: Data is stored persistently on disk. Example: RocksDB, Dynamo, Riak
    \end{itemize}
  \end{itemize}

  \subsection{Document Stores}
  \begin{itemize}
    \item Basically Hierarchical Key-Value Stores. Values are documents (JSON, XML, etc).
    \item Collections = Tables, Documents = Rows, Fields = Columns.
    \item Unlike Key-Value Stores, the database can query the values.
    \item Example Document Stores: \textbf{MongoDB}, CouchDB, Couchbase.
    \item The \$ operator represents a special key in MongoDB. It is used to project elements in an array.
    \item 1 stands for true/include, 0 stands for false/exclude in MongoDB.
    \item \includegraphics[width=0.95\linewidth]{mongodb_example.png}
  \end{itemize}

  \subsubsection*{MongoDB}
  \begin{itemize}
    \item Routers(mongos): Handles requests from application and route the queries to the correct shards.
    \item Config Server: Stores metadata and configuration settings for the cluster (e.g. which data is in which shard)
    \item Shards: Data is partitioned based on a shard key(variable of the record). Within each shard, data is stored in replica sets for redundancy and fault tolerance.
    \item Sharding allows for horizontal scalability.
    \item \includegraphics[width=0.95\linewidth]{mongo_db_read_example.png}
    \item Partition Pruning: Only the relevant shards are queried. The query is sent to the relevant shards based on the shard key.
    \item If query is not based on the shard key, the query is sent to all shards. This is called scatter-gather.
    \item Supports join queries through denormalisation. Data is duplicated across multiple collections to avoid joins.
  \end{itemize}

  \subsubsection*{Replication in MongoDB}
  \begin{itemize}
    \item Standard Configuration: 3 nodes, 1 primary, 2 secondaries.
    \item Primary receives all write operations.
    \item Secondaries replicate the primary's \textbf{operation log} and apply the operations.
    \item Users can configure the read preference to allow reading from secondaries.
    \item Secondaries have eventual consistency. They may lag behind the primary. But allowing reads from secondaries can improve decrease latency and distribute load (improving thoughput).
  \end{itemize}

  \subsection{Wide Column Stores}
  \begin{itemize}
    \item Column Families: A collection of columns. Each column family has a unique name.
    \item Sparsity: Columns can be added to a row without adding them to all rows.
    \item If a column is not used for a row, it does not use any space.
    \item Example: BigTable, HBase, Cassandra.
  \end{itemize}

  \subsection{Vector Databases}
  \begin{itemize}
    \item Each row is a vector. Each column is a dimension.
    \item Usually used to store dense, numerical and high-dimensional data.
    \item Allows fast similarity search and clustering.
    \item Major Types of algorithm: Min-Hashing, Locality Sensitive Hashing (LSH).
    \item Features: Scalability, Real-Time Updates, Replication, Fault Tolerance, High Availability.
    \item Examples: Milvus, Redis, Weaviate, MongoDB Atlas.
    \item Commonly used for AI/ML applications, especially large language models are often used to convert text into vectors which are used for search, recommendation, clustering.
    \item Similarly, vision models convert images into embeddings.
  \end{itemize}

  \subsection{Strong Vs Eventual Consistency}
  \begin{itemize}
    \item Strong Consistency: Any reads immediately after an update must give the same result on all observers.
    \item Eventual Consistency: If the system is functioning and given enough time, eventually all reads will return the last written value.
    \item RDBMS provides ACID properties (Atomicity, Consistency, Isolation, Durability).
    \item NoSQL databases often provide BASE properties (Basically Available, Soft State, Eventually Consistent).
    \begin{itemize}
      \item Basically Available: Basic reading and writing operations are available most of the time
      \item Soft State: Without guarantees, we only have some probability of knowing the state at any time.
      \item Eventually Consistent: The system will eventually become consistent.
    \end{itemize}
    \item Eventual consistency offers better availability at the cost of weaker consistency guarantee. It is suitable for applications that can tolerate stale data.
  \end{itemize}

  \section{Distributed Databases}
  \begin{itemize}
    \item Advantages: Scalability, Availability, Fault Tolerance, Lower Latency (Nearest Replica)
    \item Architectures: Shared Everything, Shared Memory, Shared Disk, Shared Nothing.
    \item RDBMS and NoSQL databases often use the Shared Nothing architecture.
    \item Network conditions between nodes may fail. In some cases, the network may be partitioned.
    \item \textbf{CAP Theorem:} Consistency, Availability, Partition Tolerance. A distributed system can only guarantee two of the three.
    \begin{itemize}
      \item Consistency: Every read receives the most recent write or an error. (Strong Consistency)
      \item Availability: Every request receives a response.
      \item Partition Tolerance: The system continues to operate despite network partitions.
    \end{itemize}
    \item Intutition: In the event of a network partition, the system can either choose to be consistent or available.
    \item Generally, NoSQL databases choose to be available and partition tolerant (AP). RDBMS choose to be consistent and partition tolerant (CP).
    \item But databases often have tunable consistency levels.
  \end{itemize}

  \subsection{Data Partitioning}
  \begin{itemize}
    \item \textbf{Table Partitioning:} Put different tables (or collections) on different nodes.
    \item Problem: Scalability is an issue as each table cannnot be split accross multiple nodes.
    \item \textbf{Horizontal Partitioning (Sharding):} Different rows (or documents) of the same table are stored on different nodes.
    \item Records are partitioned based on a shard key. The shard key is used to determine which node the record is stored on.
    \item We should choose a shard key based on the access patterns of the data (e.g. groupBy). It should be evenly distributed to avoid hotspots.
    \item We want to ultilise partition pruning to reduce the number of nodes that need to be queried.
    \item \textbf{Low Cardinality:} A shard key with low amounts of unique values. It is not suitable for sharding as it will lead to hotspots.
    \item \textbf{High Frequency:} A single shard key with high amounts of records. It is not suitable for sharding as it will lead to hotspots.
    \item To mitigate these issues, we can use \textit{composite shard keys}.
    \item \textbf{Range Partitioning:} We can combine horizontal paritioning with range partitioning. Each node is responsible for a range of shard keys.
    \item Beneficial for range queries as the query can be sent to the relevant node.
    \item \textbf{Hash Partitioning:} Each node is responsible for a range of hash values of the shard key.
    \item It can still lead to hotspots if the hash function is not well distributed.
  \end{itemize}

  \subsubsection{Consistent Hashing}
  \begin{itemize}
    \item Weakness of Hash/Range Partitioning: How do we distribute the data when we add or remove nodes?
    \item If we redo the partitioning, we will have to move a lot of data which is inefficient.
    \item \textbf{Consistent Hashing:} A technique that minimises the amount of data that needs to be moved when nodes are added or removed.
    \item Think of the output of hashing the shard key as on a circle.
    \item Each node has a marker on the circle. Each node can even have multiple markers.
    \item Each record is assigned to the node in the clockwise direction of the first marker it encounters.
    \item To delete a node, we simply remove the marker from the circle. The records that were assigned to the node will be assigned to the next node in the clockwise direction.
    \item To add a node, we simply add a marker to the circle. The records that were assigned to the next node in the clockwise direction will be assigned to the new node.
    \item Replication Strategy: Each record is assigned to the next $n$ nodes in the clockwise direction.
    \item Benefits of multiple markers: Load balancing, when we remove a node, the records are distributed to different nodes in their clockwise direction.
          \includegraphics[width=0.95\linewidth]{consistent_hashing.png}
  \end{itemize}
  
  % --------------------------------------------------------------

  \section {Apache Spark}
  \subsection{Hadoop vs Spark}

  \begin{itemize}
    \item \textbf{Hadoop}: Disk-based, MapReduce, HDFS. Not suitable for iterative algorithms as it incurs network and disk I/O overhead for intermediate data.
    \item \textbf{Spark}: In-memory, DAG, RDDs. In the event memory is insufficient, Spark spills data from memory to disk.
  \end{itemize}

  \subsection {Spark Architecture and APIs}
  \includegraphics[width=0.95\linewidth]{spark_component_api_stack.png} 
  \includegraphics[width=0.95\linewidth]{spark_architecture.png} 
  \begin{itemize}
    \item \textbf{Driver Process}: Manages the execution of the Spark job. Responds to user inputs. Distribute work to the executors.
    \item \textbf{Cluster Manager}: Manages the resources of the cluster. Eg. YARN, Mesos, Kubernetes.
    \item \textbf{Worker Node}: Runs the executors.
    \item \textbf{Executor}: Runs tasks and keeps data in memory or disk storage across them.
    \item \textbf{RDDs}: Resilient Distributed Datasets. A collection of JVM objects. Functional operators (map, filter, etc) are applied to RDDs to transform them.
    \item \textbf{DataFrames}: A distributed collection of data organized into named columns. Similar to a table in a relational database. Expression-based transformation operations. Logical plans and optimisers.
    \item \textbf{Datasets}: A distributed collection of data with a known schema. Combines the benefits of RDDs and DataFrames. Internally rows, externally JVM objects. Typs safe and fast.
  \end{itemize}

  \subsection{RDDs}
  \begin{itemize}
    \item \textbf{Resilient Distributed Datasets}
    \item Fault-tolerant collection of elements that can be operated on in parallel
    \item Immutable, partitioned collection of records
    \item Distributed across a cluster of machines.
  \end{itemize}

  \subsubsection*{Transformations and Actions}
  \begin{itemize}
    \item Transformations are operations that create a new RDD from an existing one.
    \item Lazy evaluation: Transformations are not executed immediately. They are only executed when an action is called. This allows Spark to optimise the execution plan.
    \item Example: \texttt{map}, \texttt{filter}, \texttt{flatMap}, \texttt{groupByKey}, \texttt{reduceByKey}, \texttt{join}, \texttt{order}, \texttt{select}
    \item Actions are operations that return a value to the driver program after running a computation on the dataset.
    \item Example: \texttt{collect}, \texttt{count}, \texttt{reduce}, \texttt{saveAsTextFile}, \texttt{foreach}, \texttt{take}, \texttt{show}
    \item Execution of transformations and actions are executed in parallel accross different worker machines as RDDs are distributed across different worker machines. Results are returned to the driver program in the final step.
    \item Caching: Persisting RDDs in memory across operations. Useful for iterative algorithms.
    \item \texttt{cache()} is a transformation that persists the RDD in memory.
    \item \texttt{persist(options)} is an action that allows for more control over the persistence of the RDD.
    \item \texttt{unpersist()} is an action that removes the RDD from memory.
    \item We should cache RDDs that are used multiple times in the computation or when it is expensive to recompute the RDD.
    \item If we did not cache the RDD, Spark will recompute the RDD each time it is used in an action.
    \item When worker nodes have insufficient memory, Spark may evict LRU RDDs from memory to disk.
  \end{itemize}

  \subsection{Directed Acyclic Graph (DAG)}
  \begin{itemize}
    \item A DAG is a graph with directed edges and no cycles.
    \item In Spark, the DAG is a logical representation of the computation.
    \item Transformations construct the DAG; actions execute the DAG.
    \item Spark optimises the DAG by combining operations and minimising data shuffling.
    \item Narrow dependencies: Each partition of the parent RDD is used by at most one partition of the child RDD. Example: \texttt{map}, \texttt{filter}, \texttt{flatMap}, \texttt{contains}
    \item Wide dependencies: Each partition of the parent RDD may be used by multiple partitions of the child RDD. Example: \texttt{groupByKey}, \texttt{join}, \texttt{reduceByKey}, \texttt{sortByKey}, \texttt{orderByKey} 
    \item In the DAG, consecutive narrow transformations are combined into a single stage and executed on the same machines. Wide transformations are separated into different stages.
    \item Across stages, data is shuffled across the network, which involves writing intermediate data to disk.
    \item Spark tries to minimise the number of stages and the amount of data shuffled.
    \item \textbf{Lineage}: The sequence of transformations that lead to an RDD.
    \item If a worker node fails, Spark can recompute the lost partitions of an RDD using the lineage. Note: we only need to recompute the lost partitions, not the entire RDD.
  \end{itemize}

  \includegraphics[width=0.95\linewidth, height=185px]{spark_dependency_stages.png}

  \subsection{DataFrames}
  \begin{itemize}
    \item A distributed collection of data organised into named columns.
    \item Similar to a table in a relational database.
    \item Easier to use than RDDs as it has a higher-level API.
    \item All Dataframe operations are still ultimately compiled down to RDD operation by Spark.
    \item Generally, transformation functions take in either strings or column objects.
    \item Transformations are still lazyly evaluated.
  \end{itemize}

  \subsection{Spark SQL}

  \begin{itemize}
    \item Spark SQL is a module for working with structured data.
    \item It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine.
    \item Spark SQL provides a domain-specific language for working with structured data.
    \item It allows running SQL queries on existing RDDs and DataFrames.
    \item Catalyst optimiser is the Spark SQL query optimiser. It takes a computational query and converts it into an optimised logical plan. Four Phases: Analysis, Logical Optimisation, Physical Planning, Code Generation (Project Tungsten).
    \item Multiple physical plans can be generated for a single logical plan. The optimiser chooses the best physical plan based on cost estimation.
    \item Project Tungsten is the Spark SQL execution engine. It aims to improve performance by optimising memory usage and CPU utilisation.
    \item Tungsten optimises memory usage by using binary processing, cache-aware computation, and code generation.
    \item \textbf{Unified API}: Spark SQL can be used with Java, Scala, Python, SQL, and R. It has one engine for all types of data processing.
  \end{itemize}

  \subsection{Machine Learning with Spark}

  \includegraphics[width=0.95\linewidth]{ml_pipeline.png} 

  \begin{itemize}
    \item Data Quality: Missing values (impute, drop, add column indicating it is missing or not)
    \item Categorical Encoding: Convert categorical variables to numerical variables. Numerical values are often assigned in a way that represents the ordinal relationship between the categories or inherent order among the categories.
    \item One Hot Encoding: Convert categorical variables to binary vectors. Each category is represented by a binary vector. Useful when there is no ordinal relationship between the categories, and to ensure that the categorical variable does not imply any numerical relationship.
    \item Normalization: Scale the features to a standard range. Useful for algorithms that are sensitive to the scale of the input features. Example: clipping, log transform, standard scalerm, min-max scaler.
    \item Logistic Regression: A linear model for binary classification. It models the probability that the output is 1 given the input features. Ultilses the sigmoid function. $\sigma(x) = \frac{1}{1 + e^{-x}}$
    \item $ \hat{y} = \sigma(x \cdot w + b) $
    \item Cross Entroypy Loss: Measures the difference between two probability distributions. It is used as the loss function for logistic regression. $L(y, \hat{y}) = -y \log(\hat{y}) - (1 - y) \log(1 - \hat{y})$
    \item Gradient Descent: An optimisation algorithm that minimises the loss function. It iteratively updates the weights and biases in the direction of the negative gradient of the loss function.
    \item $w_{t+1} = w_t - \alpha \nabla_w L(w_t)$
    \item Evaluation Metrics: Accuracy, Precision, Recall, F1 Score, ROC Curve, AUC.
    \item Accuracy: $\frac{TP + TN}{TP + TN + FP + FN}$
    \item Precision: $\frac{TP}{TP + FP}$
    \item Recall: $\frac{TP}{TP + FN}$
    \item F1 Score: $2 \times \frac{Precision \times Recall}{Precision + Recall}$
    \item Errors: Mean Squared Error, Mean Absolute Error, Root Mean Squared Error.
    \item Mean Squared Error: $\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2$
    \item Mean Absolute Error: $\frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y_i}|$
    \item Root Mean Squared Error: $\sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2}$
    \item R Squared Value (0 to 1): Measures the proportion of the variance in the dependent variable that is predictable from the independent variable. The higher the better.
  \end{itemize}

  \subsubsection*{Pipelines}
  \includegraphics[width=0.95\linewidth]{pipeline_model.png} 
  \begin{itemize}
    \item Benefits: Better code reuse, Easier to perform cross validation, Easier to tune hyperparameters, Easier to productionise the model.
    \item Transformers are the building blocks of a pipeline.
    \item A transformer has a transform() method that takes in a DataFrame and returns a new DataFrame.
    \item Example: VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler, LogisticRegression
    \item Generally, these transformers output a new DataFrame which append their result to the original DataFrame. 
    \item A fitted model (e.g LogisticRegressionModel) is also a transformer. It transforms Dataframe by adding a prediction column.
    \item Estimators is an algorithm that takes in data, and outputs a fitted model. Example: A learnign algorithm like LogisticRegression can be fit to data, producing the trained logistic regression model.
    \item Estimators have a \texttt{fit()} method that takes in a DataFrame and returns a Transformer.
          \includegraphics[width=0.95\linewidth]{estimator.png} 
    \item Pipelines are a sequence of stages. Each stage is either a Transformer or an Estimator.
    \item Pipeline itself is an Estimator. It has a \texttt{fit()} method that takes in a DataFrame and returns a PipelineModel.
    \item When \texttt{fit()} is called on a Pipeline, the stages are executed in order. For Transformers, the \texttt{transform()} method is called. For Estimators, the \texttt{fit()} method is called.
    \item The output of \texttt{Pipeline.fii()} is a PipelineModel, which is a Transformer, and consists of a series of Transformers.
    \item The \texttt{transform()} method of the PipelineModel applies the fitted model to the input DataFrame.
  \end{itemize}

  \includegraphics[width=0.95\linewidth]{ml_code_ex1.png}
  \includegraphics[width=0.95\linewidth]{ml_code_ex2.png} 
  \includegraphics[width=0.95\linewidth]{ml_code_ex3.png} 

  \section{Stream Processing}
  \includegraphics[width=0.95\linewidth]{stream_process.png}
  Goal: Process data in real-time as it is generated. State can be stored and accessed in many different places including program variables, local files, or embedded/external databases.

  \subsection{Micro-Batch Stream Processing}
  \begin{itemize}
    \item Spark Structured Streaming uses a micro-batch processing model.
    \item Data from the input stream is divided into micro batches, each of which will be processed in the Spark cluster in a distributed manner.
    \item Small deterministic tasks generate the output of each micro-batch. Time is divided into small intervals, and data is processed in each interval.
    \item Advantages: quick; recover from failures efficently; deterministic nature ensures end-to-end exactly-once processing.
    \item Disadvantages: latency (cannot handle millisecond); micro-batch size affects latency and throughput; micro-batch processing can be less efficient than true stream processing.
    \item It is sufficient for most use cases.
    \item In Spark Structured Streaming, the input data is treated as an unbounded table. \includegraphics[width=0.95\linewidth]{spark_stream_unbound_table.png}
    \item Five Steps to Define Steaming Query: 
          \begin{enumerate}
            \item Define the input source.
            \item Transform Data.
            \item Define output sink and output mode.
            \item Specifying processing details. (Triggering details, checkpointing, etc)
            \item Start the query.
          \end{enumerate}
  \end{itemize}

  \includegraphics[width=0.95\linewidth]{incremental_excution_stream.png}
  \includegraphics[width=0.95\linewidth]{spark_stream_fault_tolerance.png}

  \subsubsection{Data Transformation}
  \begin{itemize}
    \item \textbf{Stateless Transformations:}
        \begin{itemize}
          \item Process each row individually without needing information from previous rows
          \item Projection operations: \texttt{select()}, \texttt{explode}, \texttt{map()}, \texttt{flatMap()}
          \item Selection operations: \texttt{filter()}, \texttt{where()}
        \end{itemize}
    \item \textbf{Stateful Transformations:}
          \begin{itemize}
            \item Process each row based on information from previous rows.
            \item Example: \texttt{DataFrame.groupBy().count()}
            \item In every micro-batch, the incremental plan adds the count of new 
            records to the previous count generated by the previous micro-batch
            \item The state is maintained in the memory of the Spark executors and is 
            checkpointed to the configured location to tolerate failures.
          \end{itemize}
    \item \textbf{Stateful Streaming Aggregations:}
          \begin{itemize}
            \item \textbf{Aggregations not based on time windows:}
            \begin{itemize}
              \item Global aggregations: \texttt{groupBy().count()}
              \item Grouped aggregations: \texttt{groupBy("sensorId").mean("value")}
              \item Supported aggregations: \texttt{count()}, \texttt{sum()}, \texttt{avg()}, \texttt{min()}, \texttt{max()}, \texttt{countDistinct()}, \texttt{collect\_set()}, \texttt{approx\_count\_distinct()}
            \end{itemize}
            
            \item Processing Time: The time at which the data is processed by the system. (Not deterministic, susceptible to system delays)
            \item Event Time: The time at which the event occurred in the real world. (Deterministic)
            \item Event time decouples the processing speeed from the results. An event time window computation will yield the same result regardless of the processing time.
            \item Watermark: A threshold that determines how late the data can be in event time. Data that arrives later than the watermark is considered late data.
            
            \item \textbf{Aggregations with Event-Time Windows:}
                \begin{itemize}
                  \item Example: \texttt{sensorReadings.groupBy("sensorId", window("eventTime", windowLength, shiftAmt)).count()}
                  \includegraphics[width=0.95\linewidth]{event_time_no_overlap.png}
                  \includegraphics[width=0.95\linewidth]{event_time_overlap.png}
                  \includegraphics[width=0.95\linewidth]{spark_stream_no_watermark.png}
                  \includegraphics[width=0.95\linewidth]{spark_stream_watermark.png}
                  \item Take the latest \textbf{event time} and minus the watermark time. Let go of the rows in the result window with end interval time lower than calculated time.
                  \item Watermark just determines which window records in the table to drop off. If it was not dropped, it may still be updated even though the late data event time is before the watermark time.
                  \item Data with event time of 12:07 is still updated as the event window 
                  [12:05 $-$ 12:15] is not dropped as the window end interval time is not before the watermark time.
                  \item \textbf{Performance Tuning: }
                      \begin{itemize}
                        \item Cluster resource provisioning appropriately to run 24/7
                        \item Number of partitions for shuffles to be set much lower than batch queries
                        \item Setting source rate limits for stability
                        \item Multiple streaming queries in the same Spark application
                        \item Tuning Spark SQL Engine
                      \end{itemize} 
                \end{itemize}
          \end{itemize}
  \end{itemize}

  \subsection{Flink}
  \includegraphics[width=0.95\linewidth]{event_driven_streaming.png}
  \includegraphics[width=0.95\linewidth]{event_logs_kafka.png}
  \includegraphics[width=0.95\linewidth]{streaming_analytics_application.png}
  \includegraphics[width=0.95\linewidth]{flink_sys_architecture.png}

  \subsubsection*{Task Execution}
  \begin{itemize}
    \item A Task Manager can run multiple tasks concurrently
          \begin{itemize}
            \item Tasks of the same operator (data parallelism)
            \item Tasks of different operators (task parallelism)
            \item Tasks of different applications (job parallelism)
          \end{itemize}
    \item Task Managers offers certain number of processing slots to control the number of tasks it is able to concurrently execute. Think of slots as CPU cores.
    \item A processing slot can execute one slice of an application, i.e one parallel task of each operator of the application
    \includegraphics[width=0.95\linewidth]{flink_task_manager.png}
    \item The tasks of a running application are continuously exchanging data.
    \item Task Managers take care of shipping data from sending tasks to receiving tasks.
    \item The network component of a Task Manager collects records in buffers before they are shipped.
    \includegraphics[width=0.95\linewidth]{flink_network_component.png}
  \end{itemize}
  
\subsection{Event-Time Processing in Flink}
\begin{itemize}
  \item Flink allows for event-time processing by allowing the user to assign \textbf{timestamps} to events/records.
  \item Flink can handle out-of-order events by using watermarks.
  \item \textbf{Watermarks} are used to derive the current event time at each task in an event-time application.
  \item In Flink, watermarks are implemented as special records holding a timestamp as a Long value. Wtermarks flow in a stream of regular records with annotated timestamps.
  \item Watermarks in Flink is not directly invovled in late data handling. It is used to determine when to emit results of windowed operations.
\end{itemize}

\subsubsection{State Management}

\begin{itemize}
  \item Stateful Stream Processing Task: All data maintained by a task and used to compute the results of a function belong to the state of the task.
  \item \textbf{State Backend:}
       \begin{itemize}
        \item Local State Management: A task of a stateful operator reads and updates its state for each incoming record.
        \item Each parallel task locally maintains its state in memory to ensure fast state accesses.
       \end{itemize}
  \item \textbf{Checkpointing:}
        \begin{itemize}
          \item A Task Manager process may fail at any point, hence its storage must be considered volatile
          \item Checkpointing the state of a task to a remote and persistent storage
          \item The remote storage for checkpointing could be a distributed filesystem or a database system
          \item \textit{Consistent Checkpoints} (similar to Spark's micro-batch checkpointing, Stop the world, not used by Flink)
            \begin{enumerate}
              \item Pause the ingestion of all input streams
              \item Wait for all in-flight data to be completely processed, meaning all tasks have processed all their input data
              \item Take a checkpoint by copying the state of each task to a remote, 
              persistent storage. The checkpoint is complete when all tasks have 
              finished their copies.
              \item Resume the ingestion of all streams.
            \end{enumerate}
          \item To execute failure recovery from consistent checkpointing. Simply restart the application, reset the state of all staetful tasks to latest checkpoint, and resume tasks.
          \item \textbf{Flink Checkpointing Algorithm}
              \begin{itemize}
                \item Chandy-Lamport Algorithm: A distributed algorithm for recording a consistent global snapshot of a distributed system.
                \item Does not pause the application but decouples checkpointing from processing
                \item Some tasks contninue processing while others persist their state
                \item Uses \textbf{checkpoint barrier}, a special record that signals the tasks to persist their state
                \item Checkpoint barriers are injected by source operators into the regular 
                stream of records and cannot overtake or be passed by other records
                \item A checkpoint barrier carries a checkpoint ID to identify the checkpoint it 
                belongs to and logically splits a stream into two parts
                \item All state modifications due to records that precede a barrier are 
                included in the barrier’s checkpoint and all modifications due to records 
                that follow the barrier are included in a later checkpoint.
                \includegraphics[width=0.95\linewidth]{flink_checkpoint_buffer.png}
                \item Job manager initiates checkpoints by sending message to all sources.
                \item Sources checkpoint their state and emit a checkpoint barrier.
                \item Records from input streams for which a barrier already arrived are buffered.
                \item All other records are regularly processed
                \item Tasks checkpoint their state once all barriers have been received, then they forward the checkpoint barrier
                \item Tasks continue regular processing after the checkpoint barrier is forwarded
                \item Sinks acknowledge the reception of a checkpoint barrier to the JobManager
                \item A checkpoint is complete when all tasks have acknowledged the successful checkpointing of their state
                \includegraphics[width=0.95\linewidth]{flink_checkpoint_end_acknowledgement.png}
                \includegraphics[width=0.95\linewidth]{flink_vs_spark.png}
              \end{itemize}
        \end{itemize}
\end{itemize}

\section{Graphs}

\subsection{Graph Processing}
\includegraphics[width=0.95\linewidth]{graph_processing_tech.png}

\subsubsection{Simple PageRank}

\begin{itemize}
  \item We can visualise the web as a directed graph where each page is a node and each hyperlink is an edge.
  \item PageRank is an algorithm that measures the importance of a page in a network.
  \item The importance of a page is determined by the number of incoming links and the importance of the pages that link to it.
  \item If we assume incoming links are harder to manipulate, we can rank each page based on the number of incoming links.
  \item Problem: Malicious users can create a large number of pages that link to a target page to increase its rank.
  \item Solution: Make the rank of a page dependent on the rank of the pages that link to it.
  \item Therefore, links from important pages count more, this is true recursively.
  \item PageRank recursively computes the rank of a page based on the ranks of the pages that link to it. [Weighted edges]
  \item \textbf{Voting Formulation:}
        \begin{itemize}
          \item For each page $j$, we define its importance as $r_j$
          \item If page $j$ with importance $r_j$ has $n$ outgoing links, each link gets $\frac{r_j}{n}$ importance.
          \item Page $j$'s own importance is the sum of the votes on its incoming links.
          \item Formally: $r_j = \underset{i \rightarrow j}{\sum} \frac{r_i}{d_i}$. \newline Where $d_i$ is the number of outgoing links from page $i$.
          \item The sum of importances of pages linking to j, each divided by number of outgoing links from that page.
          \item In the event that the flow equations have no unique solution, we can add an additional constain to force uniqueness:\newline
                $r_a + r_b + r_c = 1$
        \end{itemize}
    \item \textbf{Matrix Formulation:}
          \begin{itemize}
            \item Stochastic adjacency matrix, $M$
            \item Let page $i$ has $d_i$, out-links.
            \item If $ i \rightarrow j $, then $M_{ji} = \frac{1}{d_i} $, else $M_{ji} = 0$
            \item $M$ is a column-stochastic matrix, whjen each column sums to 1.
            \item Let \textbf{Rank Vector} $= r$ and $r_i$ is the rank of page $i$.
            \item $\sum_i r_i = 1$
            \item The flow equation can be written as $r = M \cdot r$
            \item Column points to row. Each column is a page, each row is a page.
            \includegraphics[width=0.95\linewidth]{flow_equation_matrix_example.png}
          \end{itemize}
    \item \textbf{Power Iteration Method:}
        \begin{itemize}
          \item  Each node starts with 
          equal importance (of $\frac{1}{N}$). During each step, each node passes its 
          current importance along its outgoing edges, to its neighbors.
          \begin{enumerate}
            \item Suppose there are $N$ web pages
            \item Initialise: $r^{(0)} = [\frac{1}{N}, ... ,\frac{1}{N}]^T$
            \item Iterate: $r^{t+1} = M \cdot r^{(t)}$
            \item Stop when $ | r^{t+1} - r^{t} |_1 < \epsilon $
          \end{enumerate}
        \end{itemize}
    \item \textbf{Random Walk Interpretation:}
          \includegraphics*[width=0.95\linewidth]{random_walk_interpretation.png}
    \item \begin{itemize}
      \item Stationary Distribution: as $ t \rightarrow \inf $, the probability distribution approaches a steady state, representing the long term probability that the random walker is at each node, which is the PageRank scores.
      \item Three Questions:
          \begin{enumerate}
            \item Does the random walk converge? Not always
            \item Does it converge to what we want? 
            \item Are results reasonable?
          \end{enumerate}
        \item Some pages are \textbf{Dead Ends}: Pages with no outgoing links. Causes importance to leak out of the network. \includegraphics*[width=0.95\linewidth]{deadend.png}
        \item \textbf{Spider Traps}: Pages that link to each other but have no outgoing links. Eventually, all importance will be trapped in the spider trap. \includegraphics*[width=0.95\linewidth]{spider_trap.png}
        \item To solve deadends and spider traps, we cam introduce teleportation. At each step, the random walker has a small probability of teleporting to a random page. This ensures that the random walker can escape deadends and spider traps.
        \item PageRank Equation: $r_j = \underset{i \rightarrow j}{\sum} \beta \frac{r_i}{d_i} + (1 - \beta) \frac{1}{N}$
        \item $\beta$ is also known as the damping factor. It is the probability that the random walker follows a link, and $1 - \beta$ is the probability that the random walker teleports.
        \item Google Matrix A: $A  = \beta M + (1 - \beta) [\frac{1}{N}]_{N \times N} $
        \item PagreRank Equation (Matrix Form): $r = A \cdot r$
        \item In practice, $\beta$ is usually set to 0.8 to 0.9. Which allows for 5 to 10 steps on average before teleport.
        \item If at a Dead End, Always Teleport: preprocess random walk matrix $M$ and set each entry in the column of the dead-end page to $\frac{1}{N}$. This makes the matrix column stochastic. \includegraphics*[width=0.95\linewidth]{teleport_deadend.png}
      
    \end{itemize}
    \end{itemize}
    \subsubsection*{Topic Specific PageRank}
    
    Problems with Simple Page Rank:
    \begin{itemize}
      \item Measures generic popularity of a page and does not consider popularity based on specific topics. Solution: Topic-Specific PageRank.
      \item Uses a single measure of importance. Solution: Hubs-and-Authorities.
      \item Susceptible to link spamming. Solution: TrustRank.
    \end{itemize}

    \begin{itemize}
      \item Idea: Bias the random walk towards a specific topic.
      \item When random walker teleports, it picks a page from a set $S$.
      \item $S$ contains only pages that are relevant to the topicFor each teleport set $S$, we get a different PageRank vector $r_s$.
      \item \begin{equation*}
        A_{ij} =
        \begin{cases}
          \beta M_{ij} + (1 - \beta) \cdot \frac{1}{|S|} &\text{if } i \in S\\
          \beta M_{ij} + 0  &\text{otherwise}
        \end{cases}
      \end{equation*}
       \item $A$ is stoaachstic and column-normalised.
       \item \includegraphics*[width=0.95\linewidth]{topic_specific_variables.png}
       \item We can create different PageRank for different topics.
       \item Which topic ranking to use: User can pick from menu, classify query into topic, use context of query, user context.
    \end{itemize}

    \subsection*{PageRank Implementation}
    \begin{itemize}
      \item For Graph Algorithms, it involves local computation at each vertex, and communication between vertices.
      \item \textbf{Think like a vertex:} Similar to MapReduce, the user only implements a function that is applied to each vertex.
      \item The framework abstracts away scheduling and implementation details.
    \end{itemize}

    \subsubsection*{Pregel Model}
    \begin{itemize}
      \item Pregel is a distributed graph processing model developed by Google.
      \item It is based on the Bulk Synchronous Parallel (BSP) model.
      \item The computation is divided into \textbf{supersteps}. Each superstep consists of three phases: Computation, Messaging, Synchronization.
      \item \textbf{Computation:} Each vertex processes incoming messages and updates its state.
      \item \textbf{Messaging:} Vertices send messages to other vertices.
      \item \textbf{Synchronization:} All vertices synchronise and move to the next superstep.
      \item \texttt{Vertex.compute()} is the user-defined function that is called at each vertex in each superstep.
            \begin{itemize}
              \item It can read messages sent to $v$ in superstep $s - 1$.
              \item It can send messages to other verices that will be read in superstep $s + 1$.
              \item I can read or write the value of $v$ and the values of its outgoing edges. Or even, add and remove edges.
            \end{itemize}
      \item The computation is repeated until a termination condition is met.
            \begin{itemize}
              \item A vertex can choose to deactivate itself
              \item A vertex is "woken up" if it receives a message
              \item Computation halts when all vertices are deactivated
            \end{itemize}
      \item \includegraphics*[width=0.95\linewidth]{pregel_count_max.png}
      \item \includegraphics*[width=0.95\linewidth]{pregel_architecture.png}
      \item Fault Tolerance: Checkpointing to persistent storage, Failure detected through heartbeats, Corrupt workers are reassigned and reloaded from checkpoints.
      \item \includegraphics*[width=0.95\linewidth]{pregel_pagerank.png}
      \item Other Graph Processing Systems: Spark GraphX/GraphFrame ( Extends RDDs to Resilient Distributed Property Graphs, uses vertex cut), Giraph, GraphLab, PowerGraph, Trinity, Pregel+, Neo4j.
      \item \includegraphics*[width=0.95\linewidth]{spark_pagerank.png}
  \end{itemize}

\end{multicols*}

\end{document}
